'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/apis/','title':"API documentation",'content':"BlueSnap Implementation Last updated: 23/01/2020\n Purpose This document explains the implementation of BlueSnap payment processor with SampleClient web services. It describes the payment logic, provides use cases, and shows the payment sequence diagrams for each case.\nAudience The target audience of this document includes stakeholders, developers, and project managers from SampleClient and MyEnterprise.\n Overview SampleClient requires to integrate BlueSnap as an alternative payment processor for its web services. SampleClient payment processors include:\n Stripe Humboldt Paysafe Safecharge  The first stage of BlueSnap implementation with SampleClient enables payments using a credit card. This document describes the payment flow of a New Vaulted Shopper Making a Purchase.\n A New Vaulted Shopper Making a Purchase The scenario for this purchase case is the following:\n A new shopper signs up for the first-time to the SampleClient web services. The shopper decides to buy credits to have access to SampleClient content using a credit card. The shopper selects the number of credits and proceeds to the checkout form. The SampleClient payment logic selects BlueSnap as payment processor from among several processors (Stripe, Humboldt, Paysafe, and Safecharge) to make a purchase with a credit card. The shopper fills up all required fields to complete the transaction. The payment processor asks permission to store the credit card information for future transactions. The shopper clicks the Buy securely now button. The shopper receives a notification status about the purchase transaction.   Sequence Diagram The following diagram shows the payment logic for a shopper making a purchase using BlueSnap payment processor.\nFigure 1. Payment Sequence Diagram for a Shopper Making a Purchase using BlueSnap Processor\nThe process of the payment sequence for a shopper making a purchase is the following:\n The frontend sends a GET request to the backend to load the BlueSnap payment form with the Hosted Payment Fields. The backend sends a POST request to the BlueSnap API to retrieve a Hosted Payment Fields token (pfToken). The BlueSnap API returns the pfToken on the Location header. The backend returns to the frontend the following parameters: pfToken, base_url, and billing_workflow. The frontend loads the checkout form with BlueSnap Hosted Payment Fields. The shopper clicks the Buy securely now button after filling out all of the fields. The frontend sends three PUT requests to the BlueSnap API to protect customer-sensitive data (CCN, Expiration date, and CVV). The frontend receives a confirmation of the binding between the pfToken and shopper’s card information. The frontend sends a POST request to backend to make a purchase using the pfToken and shopper’s payment details. The backend sends a POST request to BlueSnap API to vault a new shopper. The backend receives a BlueSnap Customer ID associated with the new vaulted shopper. The backend sends a POST request to BlueSnap API to make a purchase using the BlueSnap Customer ID. The backend receives a notification of the payment status. The backend sends back the notification of the payment status to the frontend. The frontend displays a window with the payment status.   Endpoints The following table lists the endpoints involved in the payment logic described above:\n  From   To   Endpoint     Frontend    Backend    GET /v2/billing/creditcard      Backend    BlueSnap API    POST /payment-fields-tokens      Frontend    BlueSnap API    PUT /payment-fields-tokens/      Frontend    Backend    POST /v2/billing/creditcard      Backend    BlueSnap API    POST /vaulted-shoppers      Backend   BlueSnap API    POST /transactions    GET /v2/billing/creditcard This request enables the frontend to receive a transaction token and load the BlueSnap payment form.\nURL http://api-dev.sampleclient.com/v2/billing/creditcard/?is_mobile=1 \\ Header Parameters The following table lists the header parameters used in the request.\n  Parameter  Description  Type  Required / Optional    Authorization  Specifies the bearer token for SampleClient API authentication. Value: Bearer [USER_TOKEN].  string  required    Content-Type  Specifies the format of the content response. Value: application/x-www-form-urlencoded.  string  required    cache-control  Indicates whether there is a cache control. Value: cache and no-cache.  string  required    Query Parameters The following table lists the query parameters used in the request.\n  Parameter  Description  Type  Required / Optional    is_mobile  Specifies whether the shopper is using a mobile device. If true, value: 1.  integer  required    packages  Specifies the number of credits the shopper is buying. Value: 5, 11, 22.  integer  required    Sample Request The following is an example of a command-line request.\ncurl -X GET \\ \u0026#39;http://api-dev.sampleclient.com/v2/billing/creditcard/?is_mobile=1\u0026amp;packages=11\u0026#39; \\ -H \u0026#39;Authorization: Bearer [USER_TOKEN]\u0026#39; \\ -H \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; \\ -H \u0026#39;cache-control: no-cache\u0026#39; \\ Response Body The following table lists the elements commonly used from the response.\n  Element  Description  Type    bluesnap_pars  Contains information on the transaction token.  data object      base_url  Specifies the base API URLs for the BlueSnap environments. Values: sandbox and production.  string      hostedFieldToken  Specifies the generated transaction token.  long    rebuy  Contains information about the stored payment methods of a vaulted shopper.  array    Important:\nThe elements listed in the response body table are in blue color in the Sample Response Body.  Sample Response Body The following is an extract of a typical response showing some of the elements returned for this request.\n{ \u0026#34;creditcardform\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;multi\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;packages\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Name on Credit Card\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cc_fullname\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Street Address\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;street\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;City\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Beverly Hills\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;city\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Location \u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;225\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;id\u0026#34;: -1, \u0026#34;name\u0026#34;: \u0026#34;--------------\u0026#34; }, { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;AFGHANISTAN\u0026#34; }, ... { \u0026#34;id\u0026#34;: 239, \u0026#34;name\u0026#34;: \u0026#34;ZIMBABWE\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;country\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;ZIP/Postal Code\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;90210\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;postal_code\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;rebuy\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rebuy\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;MIC\u0026#34;, \u0026#34;value\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;fic_opt_in\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;MPlus\u0026#34;, \u0026#34;value\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;mplus_opt_in\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Check here to top up your credits when you run out so that you are ready for any encounter.\u0026#34;, \u0026#34;value\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;autobill_opt_in\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Credit Card number\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cc_number\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Credit Card Expiration Date\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;01\u0026#34; }, ... { \u0026#34;id\u0026#34;: 12, \u0026#34;name\u0026#34;: \u0026#34;12\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;cc_month\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Credit Card Expiration Date\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;id\u0026#34;: 2019, \u0026#34;name\u0026#34;: \u0026#34;2019\u0026#34; }, ... { \u0026#34;id\u0026#34;: 2068, \u0026#34;name\u0026#34;: \u0026#34;2068\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;cc_year\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Card Verification Code (CVV) #\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cc_cvv\u0026#34; } ] } POST /payment-fields-tokens This method enables to create a Hosted Payment Fields token (pfToken) when using Hosted Payment Fields. It is a server-to-server request from SampleClient to BlueSnap when the customer access the checkout form. The token serves to protect sensitive shopper information during a transaction. The transactions using a pfToken are:\n Processing a purchase. Creating a vaulted shopper. Updating a vaulted shopper with a new credit card.  Important:\nThe Hosted Payment Fields token expires after 60 minutes.  URL https://sandbox.bluesnap.com/services/2/payment-fields-tokens Header Parameters The following table lists the header parameters used in the request.\n  Parameter  Description  Type  Required / Optional    Content-Type  Specifies that the format of the response body is a JSON object.  Value: application/json.   string  required    Accept  Specifies that the format of the accepted request is a JSON object.  Value: application/json.  string  required    Authorization  Specifies the bearer token for BlueSnap API authentication. Value: Bearer [USER_TOKEN]  string  required    Sample Request The following is an example of a command-line request.\ncurl -I -X POST \\ https://sandbox.bluesnap.com/services/2/payment-fields-tokens \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;Accept: application/json\u0026#39; \\ -H \u0026#39;Authorization: Bearer [USER_TOKEN]\u0026#39; \\ Sample Response Body The following is an example of a typical response that shows all the elements of the Hosted Payment Field token generation.\nHTTP/1.1 201 201 Date: Wed, 27 Nov 2019 17:24:47 GMT Set-Cookie: JSESSIONID=XXXXXXXX; Path=/services; Secure; HttpOnly Location: https://sandbox.bluesnap.com/services/2/payment-fields-tokens/XXXXXXXX_ Content-Length: 0 Strict-Transport-Security: max-age=31536000 ; includeSubDomains Set-Cookie: XXXXXXX; Path=/ Set-Cookie: XXXXXXX; path=/services Via: 1.1 sjc1-bit32 Hosted Payment Fields Token Errors The following table lists the errors returned from a request related to the Hosted Payment Field token.\n  Code  Name  Description    14040  EXPIRED_TOKEN  Token is expired    14041  TOKEN_NOT_FOUND  Token is not found    14042  NO_PAYMENT_DETAILS_LINKED_TO_TOKEN  Token is no associated with a payment method    "});index.add({'id':1,'href':'/docs/architecture/','title':"Architecture Guide",'content':"Architecture Guide Betelgeuse Last updated: 23/11/2020\n Purpose The purpose of this guide is to describe the software architecture of the Betelgeuse application and its interaction with third-party applications.\nAudience This document is useful to the developers and stakeholders of Orion who are implementing or updating new features for the Betelgeuse application.\n Introduction Betelgeuse is a serverless application hosted in the AWS cloud. Unlike the monolithic architecture of the previous Betelgeuse system, Betelgeuse is implemented as a containerized .NET application following a microservice-based architectural style. This style decomposes the monolithic approach into three main components, each associated with a microservice: bellatrix, rigel, and saiph. Each microservice undertakes a single business responsibility from the old Betelgeuse system as follows:\n Bellatrix Microservice: Handles transactions of individuals such as create/update clients and group leaders. It also manages brands, regions, routes, and groups. Rigel Microservice: Handles transactions related to credits and payments. Saiph Microservice: Handles the communication of Betelgeuse with third-party systems and processes transaction reports.  Although microservices run independently and, therefore, are built separately, they continuously interact among themselves. The microservices are built as container images orchestrated by the AWS Elastic Container Service (ECS). AWS Fargate provisions the infrastructure of the ECS clusters. Each microservice has its own database decoupled from the other microservices. Data is migrated from the previous Betelgeuse system and stored in serverless Aurora MySQL relational databases. Moreover, communication among the microservices is asynchronous and based on integration events. It follows the principles of request-response messaging for publish-subscribe channels, using both Amazon SNS topics and Amazon SQS queues.\nBetelgeuse is deployed on the cloud using the Octopus Deploy tools. GitLab CI/CD tools ensure continuous integration and continuous delivery of the application by implementing automated pipelines. In addition, Betelgeuse uses Terraform templates to implement Infrastructure as Code on AWS.\nFinally, the Betelgeuse client application is stored in a bucket of Amazon S3 and served as a Single Page Application (SPA) using AWS Cloudfront. The components of the client application are built using the Angular Framework.\nThe Betelgeuse application comprises the following modules:\n Betelgeuse Identity and Authorization Module Betelgeuse Web Frontend Betelgeuse Backend Betelgeuse Security Betelgeuse Monitoring  Figure 1 provides a high-level description of the Betelgeuse application.\nFigure 1. Betelgeuse Architecture\nThe architectural style of Betelgeuse follows the good practices of Clean Architecture. This style is suitable for complex business logics that involve several areas. The advantages of the Betelgeuse architecture are the following:\n It follows the Dependency Inversion Principle (DIP). This principle states that high-level modules should not depend on low-level modules. The interplay between both modules must be thorugh abstractions. Therefore, high-level modules of the Betelgeuse such as data, infrastructure, dependencies, and user interface are decoupled from low-level modules (business rules). This principle focuses the Betelgeuse architecture on the business logic of Orion. It complies with a Domain Driven Design (DDD). DDD is a software development approach that links the implementation of the application to the core business concepts of the client. This approach enables to map the old monolithic architecture of Betelgeuse consisting of use cases into domains. Each domain then correlates to a microservice in the new model of Betelgeuse. It is scalable. Microservices can scale up as Orion business logic does. Each microservice can assume a single business responsibility.   Betelgeuse Identity and Authorization Module This section describes the business logic of Betelgeuse for an end user to access and navigate the application according to their role in Orion.\nAmazon Cognito is the AWS service that enables authentication and authorization of Betelgeuse end users. Cognito user pools verify the identity of users so that they can directly sign in and sign up to the application. A user pool enables Betelgeuse to create and maintain an end-user directory that includes information such as profile, company, zone, and office.\nBetelgeuse, includes two methods to authenticate an end user:\n Directly registering the new user’s profile in the AWS Cognito console. Using a third-party SAML 2.0 identity provider, such as Azure Active Directory, for integrating Office 365 single sign-on (SSO), as Figure 2 shows.  Figure 2. Authentication Flow of Betelgeuse Users using Office 365\nThe user authentication flow using a third-party identity provider is the following:\n The user enters the domain of the Betelgeuse application. The application redirects the user to the Amazon Cognito hosted UI to authenticate using Office 365 SSO. Amazon Cognito redirects the user to the Microsoft login site to enter their Office 365 credentials. If the user authenticates successfully, Azure Active Directory provides four claims related to the user profile: role, office, zone, and company to Amazon Cognito using the SAML 2.0 standard, an industry standard for federated authentication.  Note:\nOriginally, Orion’s user information was kept on a local Windows server. Eventually, the user directory was uploaded to the Azure Active Directory in the cloud using the Azure AD connect service.  Next, Cognito issues a JSON Web Token (JWT) to the Betelgeuse application using the OAuth 2.0 implicit grant flow, which enables Cognito to directly return a JWT to Betelgeuse after the user successfully authenticates. The OAuth 2.0 standard is used to control user authorization. The Amplify framework decodes and verifies the JWT. If valid, it grants the user access to the application.  Important:\nThe Betelgeuse application uses the SDKs and libraries provided by the AWS Amplify framework to integrate AWS services. Amplify also handles the Betelgeuse sign in and sign up flow by implementing direct calls to the methods of the Auth class using the SDK of Angular CLI.   Betelgeuse Frontend This section describes the elements, along with their properties and relationships, that support the client and static content of Betelgeuse. An end user interacts with these elements after successfully identifying and accessing the application.\nThe Betelgeuse application uses the Angular JavaScript framework to build a Single-Page web Application (SPA). Amazon CloudFront serves the client and static content of the application, stored in an S3 Bucket. Betelgeuse Frontend communicates to the backend microservices rigel, bellatrix, and saiph using a REST API Interface.\nA CloudFront distribution is a reliable and secure way to serve content more efficiently because it enables Betelgeuse to cache the most frequently requested content. Each CloudFront distribution uses anSSL certificate to identify the Betelgeuse site and secure its private network communications. Moreover, each deployment environment: development, laboratory, testing, and production has its own CloudFront distribution.\nFrontend DNS (Domain Name System) requests are routed to the CloudFront distribution using the Amazon Route 53 service by means of an alternate DNS domain. The CloudFront distribution points to the origin S3 Bucket that hosts the client application. The S3 Bucket is configured for website hosting with policies to grant external access. For more details, check the section Routing Traffic to CloudFront Distributions.\n Betelgeuse Backend This section describes the following topics:\n The architecture of the microservices as containerized images along with their infrastructure provisioning The routing of incoming traffic from the Frontend The structure of the Virtual Private Cloud (VPC) and subnets The request-response messaging among microservices The deployment of the Betelgeuse application   Containerized-Based Microservices Betelgeuse uses a microservice-based architecture. Each microservice is a containerized application that has its own database. Betelgeuse includes three main microservices: bellatrix, rigel, and saiph. These microservices exist in all the deployment environments of the application: development, laboratory, testing, and production.\nFigure 3. Interplay between the ECS Cluster and AWS Fargate for Building the Microservices.\nAs Figure 3 shows, each deployment environment is built on an AWS ECS (Elastic Container Service) cluster and launched using AWS Fargate. Amazon ECS is an orchestration service that handles your containers using services. Services define tasks based on a task definition, which works as a blueprint that describes how to provision your containers. Tasks are one-time executions of your containers. For example, the development environment includes an ECS cluster with three ECS services:\n orion-dev-bellatrix orion-dev-rigel orion-dev-saiph  Each service can contain a single or multiple tasks representing a container image of a specific microservice. You can scale up to any number of tasks for maintaining your application’s availability if a task failure occurs.\nAs mentioned above, the ECS service builds a container image of a microservice using a task definition. A task definition is required to run Docker images in ECS. The task definition defines some parameters such as the origin Docker image, CPU, and memory for each task, launch type, and ports. Betelgeuse uses AWS ECR (Elastic Container Registry) to store the Docker images implemented and built in GitLab by the developers. The ECS service consumes the Docker images from the AWS ECR and builds the container image of the microservices using the task definition. For more details, consult the Deployment section. To ensure compatibility and usage of the latest image version, GitLab uses semantic versioning (semver) of images.\nAWS Fargate enables you to run your containers without requiring the whole capacity of EC2 instances. Fargate handles the infrastructure required for each ECS cluster. Fargate launch type provisions your tasks with the required number of CPU cores and gigabytes of memory defined in the task definition.\n Traffic Distribution Betelgeuse’s domain name is orion.com.mx. This domain includes the following subdomains:\n dev.orion.com.mx lab.orion.com.mx test.orion.com.mx api-dev.orion.com.mx api-lab.orion.com.mx api-test.orion.com.mx  Amazon Route 53 is responsible for routing traffic to orion.com.mx and its corresponding subdomains by means of a public hosted zone. A hosted zone contains records that define how the internet traffic is routed for Betelgeuse DNS queries. Amazon Route 53 handles DNS queries and routes traffic to a specific destination of the Betelgeuse application.\nThe Amazon Route 53 service main responsibilities include:\n Mapping domain names to CloudFront distributions, Application Load Balancers and S3 Buckets. Routing traffic for a domain and its subdomains using records. Determining how to respond to DNS queries using a routing policy. Determining the format of the value that returns in response to a DNS query.  The following sections describe in detail how Amazon Route 53 maps domain names to CloudFront distributions and Application Load Balancers.\n Routing Traffic to CloudFront Distributions To describe how Amazon Route 53 distributes traffic to a CloudFront distribution, the following steps use the dev.orion.com.mx subdomain as an example.\n The frontend sends a dev.orion.com.mx DNS query. The Amazon Route 53 service routes traffic for this subdomain to the alternate domain name CNAME associated with the CloudFront distribution.  Note:\nThe DNS query name must exactly match the CloudFront alternate domain name. For example, if the alias record name is dev.orion.com.mx the alternate domain of the CloudFront distribution must be named likewise.  The CloudFront distribution points to the S3 bucket endpoint and serves the client and static content stored in the bucket.  Note:\nThe CloudFront distribution uses an SSL certificate to secure all communications among the AWS resources of the internal network. An SSL certificate is created per domain name using the Amazon ACM service.   Routing Traffic to Application Load Balancers Betelgeuse microservices are placed behind an HTTP/HTTPS load balancer secured by an SSL certificate. The load balancer routes traffic to the microservices based upon inbound rules defined on specific ports. Furthermore, the application load balancers have a security group, which acts as a virtual firewall to control inbound and outbound traffic.\nTo describe how Amazon Route 53 distributes traffic to an application load balancer, the following steps use the api-lab.orion.com.mx subdomain as an example.\n The frontend sends a api-lab.orion.com.mx/bellatrix DNS query. Amazon Route 53 routes all incoming traffic from this DNS name to the application load balancer (ALB) of the laboratory environment. The ALB redirects traffic to a microservice target group based on the route name (/bellatrix/, /rigel/, or /saiph/).  Important:\nThe ALB is configured to listen to ports: 80, 443, 8080, 8443, 8810, and 8811. ALB listeners check for connection requests that use the HTTP or HTTPs protocols on these ports. Each listener includes rules to route incoming traffic to a specific target group.  The target group contains a target of type IP. In this case, the target is the private IP address assigned to the task of the bellatrix microservice. This IP address is dynamic and changes every time the task or the service is restored.  Note:\nAll incoming traffic from ports that use the HTTP protocol is redirected to ports using the HTTPs protocol. This configuration ensures that all incoming requests have an SSL certificate to secure the connection.  For security purposes, each deployment environment has its own Virtual Private Network (VPC) that complies with the network architecture of Orion. A VPC contains three availability zones. Each availability zone includes a private and an isolated subnet for a total of six subnets. Private subnets host container images of the microservices. On the other hand, isolated subnets host databases. Besides, the ALB has an associated SSL certificate to secure communications and encrypt sent data. The Betelgeuse application has an additional security layer, the user must connect to Orion’s VPN to access the microservices resources.\nNote:\nOnly the development environment has both the microservice ECS tasks and databases configured on private subnets. Isolated subnets are not used.   Asynchronous Communication Communication among the microservices that integrate the Betelgeuse application is based on events using both Amazon Simple Notification Service (SNS) topics and Amazon Simple Queue (SQS) service. This strategy ensures a decoupled and asynchronous interaction among the different microservices that follows the principle of request-response messaging for publish-subscribe channels. In this scheme, an event bus collects all the events produced by a microservice. Then, it distributes them among the subscribed queues of other microservices, as Figure 4 shows.\nAs an example, the following steps describe in detail the communication scheme of the bellatrix microservice with the rigel and saiph microservices.\n The bellatrix microservice produces a new event. The Amazon SNS service, that listens to events from the list of topics of all the microservices, performs two actions:  It fans this event out to the list of registered topics of the rigel and saiph microservices. It processes all event logs parallelly and stores them in an S3 bucket for further reference and inspection. To accomplish that,  A Lambda function pushes the events to Amazon Kinesis Data Firehose. The Kinesis Data Firehose service captures and delivers the streaming data (events) into an S3 bucket.\nIf required, a second Lambda function can perform an identity transformation of the events in the stream.     The Amazon SQS service subscribed to the topic collects the new event. The Amazon SQS service appends the event to the queue of the rigel and saiph microservices, respectively. The rigel and saiph microservices process the event. Parallelly, the event is collected by a Death Letter Queue (DLQ) to ensure that the event is processed. A DLQ can handle duplicated events. If required, all DLQ logs can be processed by a Lambda function and stored in an S3 bucket.  Figure 4. Asynchronous and Decoupled Communication among Betelgeuse Microservices.\n Deployment Betelgeuse uses the Octopus tools for deploying the containerized images of the microservices in the cloud. Each deployment environment (development, testing, laboratory, and production) includes five containerized images. Octopus executes a PowerShell script that tells the AWS CLI which processes and resources to implement, as well as which specific version of the images stored in the AWS ECR to use. Figure 5 describes the deployment process of the Betelgeuse application.\nFigure 5. Deployment Process of Betelgeuse Environments using Octopus\nTo understand the deployment process of containerized images, the following steps describe how Octopus deploys the bellatrix image into the development environment.\n A developer makes a pull request to the Orion.Betelgeuse.Bellatrix repository. GitLab CI/CD implements automatically the following stages:  Prebuild  Determines image version using semantic versioning specification v.1.0.0 (SemVer).   Build and Test  Executes unit tests to the .NET project and all its dependencies. Builds, analyzes static code, and publishes the .NET core application and its dependencies to a folder for deployment.   Push  Builds, tags, and pushes a dockerized image of the .NET core application to the container registry of GitLab (CI_REGISTRY_IMAGE). Builds, tags, and pushes a dockerized image of the .NET core application to the AWS Elastic Container Registry (ECR). Packs the powershell deployment script into a NuGet package and pushes it to the Octopus built-in repository.   Deploy  Creates a release with the specified version in the Prebuild step and deploys it to the development environment.     Octopus runs the PowerShell script, which contains custom deployment actions to be performed by the AWS CLI.\nOctopus tells AWS ECR to deploy the bellatrix image that matches the specified version number of the created release.  Important:\nOctopus enables custom powershell scripts to have access to the AWS CLI. This requires a previous authentication step to provide AWS credentials to Octopus.   Betelgeuse Security The Betelgeuse application complies with the following security checkpoints:\n General Security Checkpoints  Users must connect to the Orion VPN to access the application. Users must have Office 365 credentials or have a username and password provided by the IT department to enter the application.\nThe IT department can create, organize, and update users directly on the AWS console of Cognito based on a role and permission matrix. Users have restricted permissions to particular Betelgeuse functionalities based on their role and the permission matrix.  Note:\nThe permission matrix applies to each endpoint of every microservice (bellatrix, rigel, and saiph) and to particular functionalities (windows) of the application’s frontend.   Architectural-Level Security Checkpoints  S3 buckets that store the client application have configured access policies. Cloudfront distributions that serve the client application have a SSL certificate. Application Load Balancers that distribute frontend DNS queries to the microservices have a SSL certificate. Each deployment environment has its own VPC including three private and three isolated subnets. Private subnets host the microservices whereas isolated subnets host the databases. Developers can connect to the isolated Amazon Aurora RDS databases from their local machine using an Amazon EC2 instance as a bastion host. This approach enables them to securely administer and give maintenance to the databases. Infrastructure as code of Betelgeuse uses Terraform templates. AWS Systems Manager Parameter Store manages secrets such as passwords, API keys, and other sensitive information (in a string format) that is consumed by the Terraform templates. All requests from the frontend to the backend using the REST API interface require a bearer token. All incoming traffic to the microservices is redirected by the Application Load Balancers to port 443 that uses the HTTPs protocol.   Betelgeuse Monitoring The AWS CloudWatch service monitors the AWS infrastructure of Betelgeuse. This service enables you to select metrics to audit AWS resources such as EC2 instances, logs, SNS messages, SQS queues, CloudFront distributions, Application Load Balancers, and so on. Additionally, Grafana is integrated in the technological stack of Betelgeuse as an open source analytics and monitoring solution to check the AWS infrastructure.\n"});index.add({'id':2,'href':'/docs/article/','title':"How-To Guide",'content':"Deploying using Octopus in Betelgeuse Last updated: 06/11/2020\n Introduction This guide explains you how to manually deploy to the AWS Cloud any of the following Betelgeuse projects using Octopus deploy:\n Orion.Client Orion.Bellatrix Orion.Rigel Orion.Saiph  Note:\nThese projects are automatically deployed to the development and testing environments. Both environments share the same projects and versions. For the laboratory environment, some of these packages are deployed manually.  The deployment process of Betelgeuse projects is detailed in the Architecture Guide. Briefly, the process consists of four stages that run automatically after a developer makes a pull request in GitLab:\n Prebuild. Determines image version. Build and Test. Builds, analyzes static code, and publishes the .NET core application and its dependencies to a folder for deployment. Push. Builds, tags, and pushes a dockerized image of the .NET core application to the container registry of GitLab and to the AWS Elastic Container Registry (ECR). It also packs the powershell deployment script into a NuGet package and pushes it to the Octopus built-in repository. Deploy. Creates a release with the specified version in the Prebuild step and deploys it to a deployment environment.  This guide describes how you can run the Deployment stage manually.\n Log In to Octopus  Go to Octopus Deploy site. Log in to Octopus.  If you are a admin user:  In the Log in page, click on the Google button to sign in using admin SSO. Identify yourself using your admin credentials.   If you are a dev user:  Identify yourself using your dev credentials.      Once you login, you are redirected to the Octopus dashboard that displays all the Betelgeuse projects and their release versions on their respective deployment environments.\n Deploy Betelgeuse Projects Manually This section describes how you can deploy Betelgeuse projects manually using Octopus GUI. As an example, the following steps illustrate the process for deploying the bellatrix microservice manually.\nSelect a Project  In the Octopus dashboard screen, select the Orion.Bellatrix project under the Default Project Group column. The application shows an overview of the project and its versions deployed in each of the deployment environments.  Create a Release  Click on the CREATE RELEASE button, located on the top left corner of the screen under the project’s name.\nOctopus loads the latest version release included in the Octopus Built-in registry pushed from GitLab. It also includes all the packages integrated in this release. Select the version and packages for the release you want to deploy. Click on the SAVE button located at the top right corner of the section. On the left panel, click on Overview.\nOctopus displays the latest release that you created in step 1.  Deploy the Release  Click on the DEPLOY… button under the environment where you want to deploy the release.\nOctopus redirects you to a new screen that shows you step-by-step the progress of the task. Click on the TASK LOG tab to check the deployment logs. Filter the deployment logs with the following parameters:  Expand = All Log level = Verbose Log fail = All Important:\nDeployment logs can help you to monitor and debug the deployment process.     In the top menu, go to Dashboard to verify that the deployment process is ready for the deployment environment you selected.  "});index.add({'id':3,'href':'/docs/howto/','title':"How-To Guide",'content':"Deploying using Octopus in Betelgeuse Last updated: 06/11/2020\n Introduction This guide explains you how to manually deploy to the AWS Cloud any of the following Betelgeuse projects using Octopus deploy:\n Orion.Client Orion.Bellatrix Orion.Rigel Orion.Saiph  Note:\nThese projects are automatically deployed to the development and testing environments. Both environments share the same projects and versions. For the laboratory environment, some of these packages are deployed manually.  The deployment process of Betelgeuse projects is detailed in the Architecture Guide. Briefly, the process consists of four stages that run automatically after a developer makes a pull request in GitLab:\n Prebuild. Determines image version. Build and Test. Builds, analyzes static code, and publishes the .NET core application and its dependencies to a folder for deployment. Push. Builds, tags, and pushes a dockerized image of the .NET core application to the container registry of GitLab and to the AWS Elastic Container Registry (ECR). It also packs the powershell deployment script into a NuGet package and pushes it to the Octopus built-in repository. Deploy. Creates a release with the specified version in the Prebuild step and deploys it to a deployment environment.  This guide describes how you can run the Deployment stage manually.\n Log In to Octopus  Go to Octopus Deploy site. Log in to Octopus.  If you are a admin user:  In the Log in page, click on the Google button to sign in using admin SSO. Identify yourself using your admin credentials.   If you are a dev user:  Identify yourself using your dev credentials.      Once you login, you are redirected to the Octopus dashboard that displays all the Betelgeuse projects and their release versions on their respective deployment environments.\n Deploy Betelgeuse Projects Manually This section describes how you can deploy Betelgeuse projects manually using Octopus GUI. As an example, the following steps illustrate the process for deploying the bellatrix microservice manually.\nSelect a Project  In the Octopus dashboard screen, select the Orion.Bellatrix project under the Default Project Group column. The application shows an overview of the project and its versions deployed in each of the deployment environments.  Create a Release  Click on the CREATE RELEASE button, located on the top left corner of the screen under the project’s name.\nOctopus loads the latest version release included in the Octopus Built-in registry pushed from GitLab. It also includes all the packages integrated in this release. Select the version and packages for the release you want to deploy. Click on the SAVE button located at the top right corner of the section. On the left panel, click on Overview.\nOctopus displays the latest release that you created in step 1.  Deploy the Release  Click on the DEPLOY… button under the environment where you want to deploy the release.\nOctopus redirects you to a new screen that shows you step-by-step the progress of the task. Click on the TASK LOG tab to check the deployment logs. Filter the deployment logs with the following parameters:  Expand = All Log level = Verbose Log fail = All Important:\nDeployment logs can help you to monitor and debug the deployment process.     In the top menu, go to Dashboard to verify that the deployment process is ready for the deployment environment you selected.  "});index.add({'id':4,'href':'/posts/','title':"Posts",'content':""});index.add({'id':5,'href':'/posts/use-chatgpt-with-python/','title':"Your Simple Guide to use ChatGPT via the OpenAI API",'content':"ChatGPT is a state-of-the-art language model developed by OpenAI. It serves many purposes in natural language processing, including language translation, chatbots, and story writing, among others.\nThis guide describes how to utilize ChatGPT through the OpenAI API service on macOS operating systems. Additionally, it provides instructions for interacting with ChatGPT using Python.\n Connect to ChatGPT via API To set up ChatGPT API follow these instructions:\n Set up an API Key Sign up to OpenAI and follow the indications to create a new secret key. All your API requests must include this API Key in the Authorization HTTP header of the request.  Important:\nKeep in a safe location your new secret key. For security reasons, you won’t be able to retrieve it again; thus, you’ll need to generate a new one.   Open terminal\n  Start making some requests using the OpenAI API client and your API Key. For further information, refer to Making Requests.\nTo request all available models, use the following request:\ncurl https://api.openai.com/v1/models \\ -H \u0026#34;Authorization: Bearer \u0026lt;YOUR_API_KEY\u0026gt;\u0026#34; To retrieve information about the text-davinci-003 model, use the following request:\ncurl https://api.openai.com/v1/models/text-davinci-003 \\ -H \u0026#34;Authorization: Bearer \u0026lt;YOUR_API_KEY\u0026gt;\u0026#34; To make a completion, use the following request:\ncurl https://api.openai.com/v1/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -H \u0026#34;Authorization: Bearer \u0026lt;YOUR_API_KEY\u0026gt;\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;text-davinci-003\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Say this is a test\u0026#34;, \u0026#34;max_tokens\u0026#34;: 7, \u0026#34;temperature\u0026#34;: 0 }\u0026#39;    Use ChatGPT with Python This section describes how to set up ChatGPT and use it in your Python scripts.\nPrerequisites Ensure you comply with the following requirements before you continue:\n Set up an API Key Install Python version 2.7 or later To check your current Python version, type the following command in the terminal python3-version  Set up ChatGPT with Python To integrate ChatGPT with your Python scripts:\n  Open terminal\n  Create a new empty project folder:\nmkdir chatgpt-python cd chatgpt-python   Install the OpenAI Python library:\npip3 install openai   Create a new file chat.pyin the project folder and edit its content:\ntouch chat.py nano chat.py   Create a completion request sample using the following template:\nimport openai # Set up OpenAI API client openai.api_key = \u0026#34;\u0026lt;YOUR_API_KEY\u0026gt;\u0026#34; # Set up OpenAI model and prompt model = \u0026#34;text-davinci-003\u0026#34; prompt = \u0026#34;Hello, how are you today?\u0026#34; # Generate a request completion = openai.Completion.create( engine=model, prompt=prompt, max_tokens=1024, n=1, stop=None, temperature=0, ) response = completion.choices[0].text print(response)   Save the chat.pyfile.\n  Make the completion request by running the chat.pyscript.\npython3 chat.py You get the following sample response:\nI am doing well, thank you. How about you? Refer to Making Requests for additional information.\n   Trusting this comes in handy for you!\n"});index.add({'id':6,'href':'/posts/top5-kaggle-competitions-2023/','title':"Secret Ingredients Found in Kaggle Competitions from 2021 to 2023",'content':"In May 2023, Kaggle launched the AI Report competition. The aim was to summarize what the ML community has learned about working and experimenting with AI over the past two years. The competition considered seven pivotal categories: text data, image and video data, tabular and time series data, Kaggle competitions, Generative AI, ethics, and others. Overall, 220 teams and 279 competitors submitted their reports, and a panel of seven Kaggle Grandmasters reviewed them to select the best of them in each category.\nIn this post, I will provide a summary of the top 5 solutions exclusively related to the Kaggle Competition category, which are indicated below. Let’s get started!\n Towards Green AI: How to Make Deep Learning Models More Efficient in Production How to Win a Kaggle Competition Kaggle AI Report: Medical Imaging Competitions A Journey Through Kaggle Text Data Competitions From 2021 to 2023 Myths Kaggle   Towards Green AI: How to Make Deep Learning Models More Efficient in Production Want to discover how Kaggle has aligned with Green AI, aiming for more sustainable AI solutions?\nIn this report, Leonie Monigatti contrasts the state-of-the-art in Green AI literature with the insights the ML community at Kaggle has gathered over the past two years to win the Efficiency Prize in Kaggle competitions. Kaggle introduced the Efficiency Prize, a dual-scoring method evaluating participants based on predictive accuracy and inference runtime, aimed at reducing the carbon footprint of Deep Learning models during inference. Various carbon reduction techniques are explored including pruning, low-rank factorization, quantization, knowledge distillation, and model conversion to ONNX format for creating lightweight models. Finally, the author suggests integrating Kaggle\u0026rsquo;s Efficiency Prize as a primary metric across all Kaggle competitions.\nHow to Win a Kaggle Competition Would you like to learn six tips that can enhance your chances of winning a Kaggle competition?\nIn this report, Darek Kłeczek conducts a two-stage meta-analysis of Kaggle write-ups from the past decade, utilizing LLMs to extract keywords related to Machine Learning methods. Based on the findings, the author discusses trends in using model ensembling methods, data augmentation techniques, and popular deep learning architectures—from CNNs to Transformers. The author emphasizes the dominant role of the Adam family of optimizers in winning solutions, along with the utilization of CE, BCE, and MSE loss functions. Lastly, the author encourages the audience to unveil the magic in their solutions by implementing labeling and post-processing techniques.\nKaggle AI Report: Medical Imaging Competitions Are you curious to know the interplay between medical imaging modalities, deep learning, and Kaggle competitions?\nIn this report, Nghi Huynh provides a solid background suitable for all audiences on medical imaging-related tasks, including object detection, classification, and segmentation that use imaging modalities such as MRI, CT, and X-rays. Moreover, by conducting a meta-analysis of the top 10 write-ups of 11 Kaggle medical imaging competitions over the past five years, the author describes the evolution of deep learning models from traditional and reliable methods such as CNNs to newly emerging ones such as Vision Transformers.\nA Journey Through Kaggle Text Data Competitions From 2021 to 2023 Have you ever wondered if the top 3 solutions in text-oriented Kaggle competitions share one or more secret ingredients? What do some Kagglers refer to as \u0026lsquo;the magic\u0026rsquo;?\nIn this report, Liliana Badillo and Salomon Marquez have conducted an in-depth analysis of 27 write-ups corresponding to nine text-oriented Kaggle competitions. In a practical and results-driven manner, the authors guide us to comprehend four widely used concepts in text competitions that have ensured an improvement in the Private Leader (PB) score in winning solutions: model architectures, pseudo labeling, adversarial weight perturbation, and mask language modeling. Furthermore, the authors analyzed the main questions that Kagglers have when implementing these strategies in their solutions.\nMyths Kaggle Have you ever wondered how Kaggle competitions have evolved over the past 3 years?\nTim Riggins debunks the myths and stereotypes that have emerged during the evolution of Kaggle competitions. The author employs a highly conversational tone, to discuss the following topics: level stacking vs. model averaging, solo vs. team-based solutions, PB score analysis for top solutions, overcoming data leaks, and the shifting participation dynamics between old-school and debutant Kagglers.\nConclusion In summary, the Kaggle AI Report competition of 2023 provided valuable insights into the trends and innovations in Machine Learning. This post focused on the top 5 solutions within the Kaggle Competition category, each offering valuable perspectives on domains such as Green AI, medical imaging, best practices handling text data, and the evolution of Kaggle competitions over time, showcasing the collaborative spirit and continuous innovation of the ML community. For further information about the top essays created for this competition, please refer to the Kaggle AI report 2023.\n"});index.add({'id':7,'href':'/posts/fundamentalsofdictionaries/','title':"Fundamentals of Dictionaries",'content':"Are you interested in learning about a more efficient way to store and access information than using a traditional list? Consider using dictionaries. A dictionary, also known as an associative array, map, hash, or hash map, is a data structure that represents objects as key-value pairs, making it easy to access information by knowing a key.\nFor instance, suppose you want to create an inventory of your video games, including information like name, genre, platforms, and ratings. Instead of using a list and scanning through it to find a particular video game feature, you can define the name of the video game as a key and the rest of its information as values in a dictionary, as shown in Fig. 1. This way, you only need to look up the name of the video game (key) to access its information (values).\nFig. 1 A sample dictionary that includes three key-value pairs from a video game collection\nWhat differentiates a dictionary from a list? Uniqueness and order. Dictionaries have keys rather than indexes and each key is unique. Generally, keys are integer or string data types. Values, on the other hand, are not unique and can be of any data type, as shown in Fig. 1. Because dictionaries lack indexes, elements in a dictionary cannot be sorted out. Therefore, when you add a new key-value pair to a dictionary, it is allocated with no intrinsic order.\nWhat are the main operations on dictionaries? Although the syntax and features of dictionaries can vary across programming languages, typically you can perform the following tasks:\n Lookup a value using its key Insert a new key-value pair Update a value Remove a key-value pair  What are some issues with dictionaries? Dictionaries are quite efficient for lookup and insertion operations. They can get the same performance as lists by turning keys into list indexes using a hash function. However, when processing large data, dictionaries can consume a lot of resources because they are prone to allocating more memory than needed. Therefore, it is essential to consider your use case scenario before using dictionaries to store and access information.\nConclusion Unlike traditional lists, dictionaries represent an efficient way to store and access a collection of objects using key-value pairs. If you’re interested in learning more about dictionaries, you can consult the following resources:\n Fundamentals of data structures: Dictionaries What’s the use of a Dictionary Data Structure? — With JavaScript Dictionaries and Sets  "});index.add({'id':8,'href':'/posts/airbyte-azure-vm/','title':"Deploy Airbyte on a Microsoft Azure VM",'content':"Disclaimer:\nI created this document based on an open source contribution to Airbyte. Please refer to the source document at Deploy on Azure   This page guides you through deploying Airbyte Open Source on a Microsoft Azure VM by setting up the deployment environment, installing and starting Airbyte, and connecting it to the VM.\nNote:\nThe instructions have been tested on a standard DS1 v2 (1 vcpu, 3.5 GiB memory) Microsoft Azure VM with Ubuntu 18.04.  Set up the environment Install Docker and Docker Compose in the VM:\n  Create a new VM and generate the SSH keys to connect to the VM. You’ll need the SSH keys to connect to the VM remotely later.\n  To connect to the VM, run the following command in the Azure Cloud Shell:\nssh \u0026lt;admin username\u0026gt;@\u0026lt;IP address\u0026gt; If successfully connected to the VM, the working directory of Cloud Shell should look like this: \u0026lt;admin username\u0026gt;@\u0026lt;virtual machine name\u0026gt;:~$\n  To install Docker, run the following commands:\nsudo apt-get update -y sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release -y curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \u0026#34;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli -y sudo usermod -a -G docker $USER   To install Docker Compose, run the following command:\nsudo wget https://github.com/docker/compose/releases/download/1.26.2/docker-compose-$(uname -s)-$(uname -m) -O /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose   Check Docker Compose version:\ndocker-compose --version   Close the SSH connection to ensure that the group modification is considered:\nlogout   Reconnect to the VM:\nssh \u0026lt;admin username\u0026gt;@\u0026lt;IP address\u0026gt;   Install and start Airbyte Download Airbyte and deploy it in the VM using Docker Compose:\n  Ensure that you are connected to the VM:\nssh \u0026lt;admin username\u0026gt;@\u0026lt;IP address\u0026gt;   Create and use a new directory:\nmkdir airbyte cd airbyte   Download Airbyte from GitHub:\nwget https://raw.githubusercontent.com/airbytehq/airbyte/master/{.env,docker-compose.yaml}   Start Airbyte by running the following command:\nsudo docker-compose up -d This step takes about two minutes to complete. When done, you will see the cursor prompt.\n  Connect to Airbyte Test a remote connection to your VM locally and verify that Airbyte is up and running.\n  In your local machine, open a terminal.\n  Go to the folder where you stored the SSH key.\n  Create a SSH tunnel for port 8000 by typing the following command:\nssh -N -L 8000:localhost:8000 -i \u0026lt;your SSH key file\u0026gt; \u0026lt;admin username\u0026gt;@\u0026lt;IP address\u0026gt; As a result, nothing happens. The cursor prompt keeps blinking.\n  Open a web browser and navigate to http://localhost:8000. You will see Airbyte’s landing page.\n  Important:\nFor security reasons, it is strongly recommended not to expose Airbyte on Internet available ports. Future versions will add support for SSL and authentication.  Troubleshooting If you encounter any issues, reach out to our community on Slack.\n"});index.add({'id':9,'href':'/posts/bigqueryvsredshift/','title':"What Are Two Key Differences Between BigQuery and Redshift?",'content':"Big Data. We often hear about this word but we aren’t quite sure how to define it. Literature says that having more than 1 TB of data can be considered as Big Data. After getting a mild idea of the concept, our next question would be: How can we manage such a large amount of data to find useful insights out of it? Well, one approach that can answer this lies in data warehousing.\nA data warehouse is a data management system for storing and analyzing historical data. It enables business intelligence (BI) to perform queries and analysis on data. From renewable energy (General Electric) and clothing industry (American Eagle) to long-distance carpooling platforms (BlaBlaCar), enterprises rely on data warehouses to help them understand their customers’ experience by analyzing their logs over time.\nIn this article, we’ll compare the architecture and price of two of the major data warehouses currently leading the data market: AWS Redshift and BigQuery. Let’s get started!\n Why Is Columnar Data Storage Important for Data Warehouses? Accountants often use spreadsheets to report and keep track of their clients’ financial statements. By sequentially filling out data in rows, accountants can store from thousands to millions of financial entries. As this information grows, it can take up a lot of disk storage, and most importantly, when accountants perform any analysis on this data, reading each entry row-by-row ends up in a very time-consuming task.\nData warehouses, on the other hand, store data more efficiently in columns rather than in rows. The columnar data storage allows them to keep data closer together reducing seek time. A columnar design is also advantageous for preparing data for further distribution on several processing units. Data warehouses split up data and processing power among multiple compute resources that execute and coordinate operations simultaneously and in parallel, a process called massive parallel processing (MPP).\nThe row and columnar data design introduce two data processing capabilities: online transaction processing (OLTP) and online analytical processing (OLAP). OLTP transactions typically involve most or all of the columns in a row. Examples of this type are banking and ATM transactions that store data as a single record in a row. On the other hand, OLAP transactions read only a few columns from a very large number of rows. As you will see in the following section, this is the type of transactions that are involved with data warehouses.\n Using Redshift or BigQuery, Does It Matter? Now that you have a quick refresher of the fundamental concepts about data warehouses, let’s compare Redshift and BigQuery, which are two popular columnar storage-oriented data warehouses. In this section, we’ll help you decide which one to go with in terms of their architectural design and cost.\n How Does Redshift Architecture Differ From That of BigQuery? Let’s review the architectural styles of both data warehouses.\nRedshift AWS Redshift is a managed warehousing solution that is based on PostgreSQL. This is why it can be considered as a Relational Database Management System (RDBMS) compatible with most SQL clients applications and powerful enough to handle OLTP operations over rows. However, Redshift infrastructure goes far beyond because it is designed and optimized to manage Big Data workloads. In Redshift, you can create tables but their storage format will be slightly different from that of PostgreSQL. This is because Redshift stores data in a columnar format that improves query efficiency and speed.\nThe core of Redshift are clusters. A cluster is a collection of compute nodes. When a cluster includes two or more compute nodes, Redshift leverages a third node, known as the leader node, to orchestrate the incoming workload, see Figure 1. This node is the main point of contact between the client application and the compute nodes. The main responsibilities of the leader node are:\n Elaborate and execution plan Distribute data and workloads across compute nodes Gather results from compute nodes  Figure 1. AWS Redshift architecture. Reference from AWS resources.\nThe capacity of a Redshift cluster is determined by the number of compute nodes and the node type. Each compute node has its own compute resources such as CPU, memory, and attached storage. The compute nodes redistribute compute resources across node slices and thus each node slice can have a compute power of a single CPU. All communications across nodes are handled through an internal AWS network.\nLikewise resources, Redshift apportions user data in tables following a styling distribution scheme. Besides, you can select a column in a table that decides upon the sorting order of your data by using a sort key. This practice leverages data throughput and efficiency.\nThe interaction between AWS Redshift components resembles a Poker game played in teams. The dealer (lead node) is the point of contact between the cassino owner and the customers. Let’s say that the dealer sorts and distributes a deck of five cards across three teams, each one is led by a team leader (compute node) and composed by three members (node slices). Then, the team leader can redistribute the five cards among their teammates based on the card’s suit, color, or number (distribution style). When it is time to expose and compare hands to determine a winner, the leaders gather all cards (aggregates) from their teams and give them back to the dealer. Finally, the dealer notifies the house who is the winner.\nBigQuery BigQuery is a serverless data warehouse solution. Serverless means that you don’t need to worry about provisioning your infrastructure, BigQuery takes care of allocating resources on your behalf. Interestingly, its architecture decouples compute from storage, which means that you first have to bring your data, of any size, and then analyze it using standard SQL syntax. Under the hood, BigQuery employs low-level Google technologies such as Dremel, Borg, Colossus, and Jupiter to operate, as Figure 2 shows.\nFigure 2. High-level architecture of BigQuery. Reference from Panoply\nLet’s provide a summary of each technology:\nDremel. It handles computations and executes SQL queries. Depending on your query size and complexity, Dremel automatically calculates how many slots are required. A slot is a virtual CPU that represents the fundamental compute element of BigQuery. Dremel dynamically distributes queries from the client application to slots following an execution plan that resembles a tree structure. As Figure 2 shows, the execution plan starts on a root server (trunk) that apportions the workload across mixer nodes (branches). Mixers, in turn, redistribute the workload among leaf nodes (leaves), and so on.\nBorg. It is an orchestrator that provisions root servers, mixer nodes, leaf nodes, and slots with hardware resources.\nColossus. It is the global storage system. Colossus servers store data in a columnar storage format and use a compression algorithm for handling Big Data optimally. Colossus’ primary tasks include fetching data to leaf nodes and handling data replication and recovery.\nJupyter. It is a Petabit network that establishes communication between compute and storage resources.\nTo understand BigQuery’s architecture, we can use a restaurant metaphor. You as the customer (client application) order a meal from the menu. The waiter (root server) takes your order to the chef (mixer). The chef then apportions your order across three groups of cooks (leaf nodes). Each group is responsible for preparing a specific component of your dish: salad, soup, and red meat. To start, the cooks need to gather all the ingredients from a big fridge (global storage system) located in another section of the kitchen. When each portion of your order is ready, the chef gathers each element, validates its flavour and quality, and turns it into a nice looking dish (aggregates). The chef passes on your meal to the waiter. Finally, the waiter serves the dish on your table. Bon appetit!\nAnswer How does Redshift architecture differ from that of BigQuery? Primarily, Redshift is a node-based solution and BigQuery is a serverless solution. This translates into ease of use for the end-user. The following table shows the main architectural differences for both data warehouses:\n  Redshift   BigQuery    It’s a node-based data warehouse  It's a serverless data warehouse solution    Couples compute and storage resources  Decouples compute and storage resources    Follows a top-down node hierarchy to execute queries  Follows a top-down slot hierarchy to execute queries    Requires to define beforehand the node type and number of nodes for your cluster unless you turn on concurrency scaling (might incur extra charges)  Scales up slots automatically based on on-demand workloads     Is Redshift More Expensive Than BigQuery? Let’s look at how the different architectures affect the cost of both solutions.\nRedshift As you can see in the previous section, AWS Redshift is a node-based data warehouse. Therefore, if you want to estimate the cost of your cluster, you need to determine beforehand the number of nodes and node types that your job will require. Determining the node size of your cluster is beyond the scope of this article, but you can find more information in About clusters and nodes. Let’s discuss about the three node types that AWS Redshift has:\n Dense Computing (DC2). Let’s consider the scenario where your data requires thousands of joins and aggregations of some of your results. Noticeably, these operations will consume a lot of compute resources. DC2 nodes are optimized to handle this type of compute-intense operations by providing compute resources with attached SSD storage. If your workload is not meant to grow, consider this node type as your best choice, otherwise you’ll need to add more nodes manually in order to store your results. Dense Storage (DS2). With this node you can create data warehouses that sit on hard disk drives (HDDs) resources. AWS recommends the usage of RA3 instead of DS2 nodes because of their better performance and cost. RA3. Perhaps you prefer an infrastructure that allows you to choose the number of nodes that you require and handle for you your storage resources. RA3 nodes fit into this category by implementing a managed storage solution that is independent from the compute resources. It takes away the burden of using DC2 nodes. RA3 nodes are optimal for BigData applications that require a storage solution that might scale up. In the event that the capacity of the SSD drive attached to the node is exceeded, RA3 nodes can scale their storage capacity by mananing offloads to S3 buckets without extra charges for using AWS S3.  The following table shows the cheapest node that you can spin up from each node type category. Extracted from AWS.\n  Node Type  vCPU  Memory  Addressable storage capacity  I/O  Price    Dense Compute DC2              dc2.large  2  15 GiB  0.16TB SSD  0.60 GB/s  $0.25 per Hour    Dense Storage DS2              ds2.xlarge  4  31 GiB  2TB HDD  0.40 GB/s  $0.85 per Hour    RA3 with Redshift Managed Storage              ra3.xlplus  4  32 GiB  32TB RMS  0.65 GB/s  $1.086 per Hour    Once you have chosen the node type and number of nodes for your cluster, you can take the most out of AWS pricing models:\n On-demand pricing model. Use this payment scheme under the basis of paying for what you use. There are no strings attached. Once you are done processing your data workload, you just simply turn off your cluster to avoid extra costs. This principing model is hourly rated, it follows the list of prices per node type described in the table above. So, the cheapest node you can run will cost you $0.25 per hour. Reserved Instances pricing model. If you consistently plan ahead your workloads, let’s say you run a job on a timely-basis, you can reserve nodes for one or three years. With this payment model, you can save up to 75% on on-demand prices ($0.0625 per hour for the cheapest node) when choosing for example an upfront payment option. If you have a limited initial budget, you can also explore other alternatives such as no upfront and partial upfront payment schemes. Spot instances pricing model. In case that you have spiky workloads runned at no particular schedule, you can consider this pricing model for your cluster. Let’s say that you need to extract, transform and load a batch of your data into an S3 bucket by the end of the day. This job takes you around 10 to 15 min. For this type of operation, AWS can offer you spare infrastructure for a short timeframe saving up-to 90% on on-demand prices ($0.025 per hour for the cheapest node). However, you should keep in mind that AWS can claim back these resources at any time.  BigQuery BigQuery adds one layer of complexity to its pricing model because it splits up compute from storage resources. If you want to estimate your bill, keep in mind the following components of BigQuery pricing:\n Analysis. It is the cost of running queries, scripts, or functions. BigQuery offers two schemes: on-demand and flat-rate pricing. The on-demand scheme allows you to pay for the number of bytes processed by each query at a cost of $5 per TB. The amount of data processed by queries includes all selected data per column. Under this scheme, the slots units are not dedicated, which means that BigQuery can share this infrastructure with other clusters. If, on the other hand, you want to provision your cluster with dedicated infrastructure, the flat-rate pricing is your best choice. This is a region-oriented pricing scheme where you commit to the use of resources for a specific period of time:  Per second. You pay for the use of 100 slots for $2920 per month. The commitment duration is only 60 seconds. You can cancel anytime once your commitment purchase is successful. Keep in mind that slots availability is not guaranteed. Per month. You pay for the use of 100 slots for $2000 per month. **Per year. **You pay for the use of 100 slots for $1700 per month.   Storage. It is the cost of storing your data. BigQuery offers two schemes: active and long-term storage. The difference between both schemes relies upon the storage time frame. If you modify a table or partition of a table in the last 90 days (active storage), BigQuery charges you $0.02 per GB. In case that you exceed 90 consecutive days without doing any modification on your tables (long-term storage), BigQuery offers you a 50% discount on the active storage. For more details check Data size calculation. Extras. It includes costs related to data ingestion and extraction.  Answer Is Redshift more expensive than BigQuery? Well, It depends on your use case. Here are the main takeaways about costs for both data warehouses:\n  Redshift  BigQuery    Costs depend on the node type and number of nodes per cluster  It has two main pricing components: analysis and storage    Node types include DC2, DS2, and RA3 nodes.  Storage component comprises active and long-term storage pricing. You pay $0.02 per GB for modified tables in the last 90 days.    Nodes have hourly rates. The cheapest DC2 node you can spin up will cost you $0.25/h.  Analysis component comprises on-demand and flat-rate pricing. You pay $5 per TB for the number of bytes processed by each query    You can find great deals by reserving instances using upfront, no upfront, and partial upfront pricing schemes.  You can find great deals by committing to the use of resources for a specific period of time: per second, per month, and per year.     So, Which Data Warehouse Is Better To Use? We don’t have in fact an actual winner out of this comparison article. Both Redshift and BigQuery are data warehouses that have gained their status as leading products in the current data market. However, through our analysis of their architectural style and cost, we have identified differences that can help you decide when to choose a data warehouse solution based on your needs.\nIn our opinion, Redshift’s architecture provides greater flexibility to end-users to fine-tune resources according to their needs, which can impact on performance and cost. However, Redshift end-users should have an intermediate technical knowledge base to provision compute nodes by choosing the correct node type and the right number of nodes. In terms of cost, Redshift establishes hourly rates per cluster, which is quite cost-effective for programmed workloads like ETLs.\nOn the other hand, BigQuery’s serverless solution brings simplicity. Provisioning of resources is on BigQuery’s behalf, which can benefit entry-level end-users who only need to upload their data and start creating SQL queries. Because BigQuery decouples storage and compute, estimating costs based on its pricing model is rather complex. However, we believe that BigQuery is optimal for spiky workloads because it charges you per processed data instead of having to pay a full hour.\nWe encourage you to take a step further this fundamental understanding of both data warehouses. Create a free account and get hands-on experience with AWS and GCP free tier.\n"});index.add({'id':10,'href':'/posts/pythonpackages/','title':"How to Create Python Packages",'content':"A Python package is a collection of modules. A module is the easiest way to import and reuse existing Python code that’s developed by other programmers. In simple terms, a package is a module, but not all modules are packages. So, the question that raises is: what differentiates a package from a module? The response lies in a file named __init__.py, which indicates that the folder directory is a package. In this article, we’ll walk you through the main concepts and steps about how to create, install, publish, and test your own Python package locally and globally.\nTo start, we’ll define a function that calculates the square of a given number. The “squarenum.py” file contains the function script, as shown in the code snippet below:\ndef calculate_square(num): \u0026#34;\u0026#34;\u0026#34; Function that returns the square of a number Args: num(float): a number Returns: float: square of the number \u0026#34;\u0026#34;\u0026#34; return num**2 Our goal is to turn this function into a Python package. To do so, we first guide you step-by-step how to create your package locally. In doing so, you will learn how to structure your package’s directory, install it and test it. Finally, we’ll provide insights about how to publish your package into the Test PyPi and Pypi third-party software repositories for Python to make your package available worldwide.\n Local Distribution of a Python Package Let’s first look at how you can structure, create, and install your Python package locally.\nPrerequisites Here’s what you’ll need to get started and creating your package distribution for local installation in macOS:\n First, ensure that you already have pip installed. Pip is a package manager. To check if you have pip, run the following command in your terminal:  pip -V Next, create a virtual environment to safely install your package. Using a virtual environment will simplify the task of installing and removing your package without affecting your current Python configuration. Luckily, Python3 and later versions come with a pre-installed environment manager called venv. For this example, you’ll create a virtual environment named “myenvironment.” Go to your main project’s directory and run the following command:  python3 -m venv myenvironment As a result, a new folder named “myenvironment” is created containing a brand new Python virtual environment.\nStructuring Your Package Folder To create your local package, here’s the structure that your package will need to follow:\n└── squarenum_distribution ├── setup.py └── squarenum_package ├── __init__.py └── squarenum.py This table provides a detailed description of each file:\n  File   Description    squarenum_distribution  Main project’s directory.    setup.py  Python file required for pip installing your package distribution.    squarenum_package  Parent package’s directory. Note that the folder’s name will be the name of your package.    __init__.py  Python file executed when importing the package. It is required to import the directory as a package. Remember to leave this file empty.    squarenum.py  Python file that contains the logic of your package, in this case, your function.    Let’s look at how we can scaffold your package’s folder.\n To start, go to your terminal and create the two main folders “squarenum_distribution” and “squarenum_package” using the “mkdir” command. Here’s how you can do it:  mkdir squarenum_distribution Next, you’ll need to create the rest of the files in their corresponding folders by simply running the “touch” command. Here’s an example you can follow:  cd squarenum_distribution/squarenum_package touch setup.py Once you\u0026rsquo;re done making all the files for your package’s directory, you’ll need to modify and include additional content for the “setup.py” and “squarenum.py” files. We recommend using the “nano” command in your terminal for this task. Alternatively, you can use your text editor of choice. With that said, ensure to include the following script for “setup.py”:  nano setup.py from setuptools import setup setup(name=\u0026#39;squarenum_package\u0026#39;, version=\u0026#39;0.1\u0026#39;, description=\u0026#39;Square of a number\u0026#39;, packages=[\u0026#39;squarenum_package\u0026#39;], zip_safe=False) Note that the “name” and “packages” parameters inside the setup() function correspond to the name of the “squarenum_package” folder that you created in step 1. In other words, these parameters are named likewise your package folder. Keep that in mind!.\nTo learn more about the contents of the “setup.py” file, check out this link.\nNext, you’ll need to repeat step 3 to include your function script in the “squarenum.py” file, as shown below:  nano squarenum.py def calculate_square(num): \u0026#34;\u0026#34;\u0026#34; Function that returns the square of a number Args: num(float): a number Returns: float: square of the number \u0026#34;\u0026#34;\u0026#34;\treturn num**2 Finally, go to your main package’s folder ./squarenum_distribution and type the “tree” command:  cd squarenum_distribution tree As a result, you’ll obtain the structure of your package that we were looking for.\n├── setup.py └── squarenum_package ├── __init__.py └── squarenum.py Installing and Testing Your Package in a Virtual Environment After structuring your package folder, you’ll need to install and test your package distribution locally using a virtual environment:\n In the console, go to the root folder that contains your “myenvironment” virtual environment. Ensure you are outside the “myenvironment” folder. Activate your virtual environment using the following command:  source myenvironment/bin/activate Note that your command line’s prompt has changed. It should now be preceded by the name of your virtual environment, as shown below:\n(myenvironment) [user]@[hostname]:[current_directory]$ With the virtual environment activated, go to the ./squarenum_distribution folder and run the following command:  cd squarenum_distribution pip install . You will get the following output, which indicates that your package has successfully installed in your virtual environment:\nProcessing ~/squarenum_distribution Installing collected packages: squarenum-package Running setup.py install for squarenum-package ... done Successfully installed squarenum-package-0.1 Now, let’s import our brand new Python package. In the console, open a Python interpreter:  python Import your package distribution by running the following command:  from squarenum_package import squarenum Here “squarenum” is the Python file that contains your function logic and “squarenum_package” is your package name.\nFinally, test your function using the following command: \\  squarenum.calculate_square(7) 49 Note that we used the dot notation to call the “calculate_square()” function inside the “squarenum” file. The dot notation means that you’re referring to something inside a file.\nInstalling and Testing Your Package as Part of Your Python Environment To install and test your package distribution locally as part of your Python environment, just repeat the same steps from the previous section but without using a virtual environment. Keep in mind that any changes you make to your system-level Python setup will be more difficult to undo.\n Global Distribution of a Python Package Let’s take our local package distribution a step further. We’ll now make our package available in a repository so that other users can download and use it. In other words, we’ll make our package globally. We’ll be using the Python Package Index, abbreviated as Pypi, which is the official third-party software repository for the Python programming language.\nWe’ll start by publishing our package into Test Pypi to check whether everything in our package is working fine. And then, as the cherry of the cake, we’ll publish it into PyPI to make it officially available to the Python community.\nPrerequisites The instructions for creating a global package somewhat differ from those for a local package, but are still quite straightforward. As a starting point, you’ll ensure to comply with the following:\n First, check whether you have already installed pip, run the following command in your terminal:  pip -V Next, install twine. This is a utility for publishing Python packages on PyPI. In your terminal, type:  pip install twine As the next step, create a virtual environment in your system to safely install your package. For this section of the article, we’ll create two virtual environments: one for the Test Pypi, and another for Pypi. To do so, go to your main project’s directory and run the following command:  python3 -m venv mytestpypi_environment python3 -m venv mypypi_environment As a result, you’ll have two new folders named “mytestenvironment” and “mypypienvironment” containing a brand new Python virtual environment.\nFinally, create a Test Pypi and a Pypi account.  Structuring Your Global Package Folder Here’s the structure that your package will need to follow:\n└── squarenum_distribution_Pypi_test ├── setup.py └── squarenum_package ├── README.md ├── __init__.py ├── license.txt ├── setup.cfg └── squarenum.py Note that the global package folder structure will slightly differ from that of a local package. You’ll notice three new files: “README.md,” “license.txt,” and “setup.cfg.” Since your package will be globally available, you’ll need a signature that tells the world: “Hey, this masterpiece was created by me!” Also, it is recommended to provide your end-users with instructions of how to install and run your package distribution. Here’s where the three new “amigos” come into play. The following table provides an overview of each file.\n  File   Description    squarenum_distribution_Pypi_test  Main project’s directory.    setup.py  Python file required for pip installing your global package distribution.    squarenum_package  Parent package’s directory. Note that the folder’s name will be the name of your package.    README.md  File that contains information and instructions of how to use your package.    __init__.py  Python file executed when importing the package. It is required to import the directory as a package. Remember to leave this file empty.    license.txt  Text file that contains information about your global package’s licensing. Most package creators follow the MIT license template.    setup.cfg  File containing your global package’s metadata.    squarenum.py  Python file that contains the logic of your package, in this case, your function.    Let’s now look at how to structure your global package’s folder.\n As a cheatsheet, we can reuse the instructions provided in the Structuring your Package Folder section, where the commands “mkdir” and “touch” were very handy. After creating all of the files for your global package’s directory, you’ll need to modify and include additional content for the “setup.py,” “setup.cfg,” and “squarenum.py” files. We again recommend using the “nano” command in your terminal for this task. Include the following script for the “setup.py:”  nano setup.py from setuptools import setup setup(name=\u0026#39;squarenum_package\u0026#39;, version=\u0026#39;0.1\u0026#39;, description=\u0026#39;Square of a number\u0026#39;, author=\u0026#39;Salomon Marquez\u0026#39;, author_email=\u0026#39;phd.smarquez@gmail.com\u0026#39;, packages=[\u0026#39;squarenum_package\u0026#39;], zip_safe=False) Note that we included two additional parameters inside the setup() function “author” and “author_email.” If you want to understand a bit more about the content of the “setup.py” file, you can consult this link.\nNext, you’ll need to include the following script into the “setup.cfg” file:  nano squarenum.py [metadata] description_file = README.md Finally, include your function script in the “squarenum.py” file, as shown below: \\  nano squarenum.py def calculate_square(num): \u0026#34;\u0026#34;\u0026#34; Function that returns the square of a number Args: num(float): a number Returns: float: square of the number \u0026#34;\u0026#34;\u0026#34;\treturn num**2 Publishing Your Package to the Test Pypi Repository With your global package folder structured, you can now publish your package to the Test Pypi repository. Here’s how to upload it.\n In your console, go to ./squarenum_distribution_PyPi_test and run the following command:  cd squarenum_distribution_PyPi python setup.py sdist Then, upload your package distribution to Test PyPi by running:  twine upload --repository-url https://test.pypi.org/legacy/ dist/* When prompted, enter your Test Pypi account username and password. Recall that creating a Test PyPi account was a prerequisite. This is the right time to create one if you have not already done so.  Uploading distributions to https://test.pypi.org/legacy/ Enter your username: Enter your password: In your browser, log in to your Test PyPi account. Finally, go to your projects section and check that your global package distribution has been uploaded.  Important:\nIf your global package distribution name contains an underscore symbol, this is changed into hyphen when it is uploaded to the Test PyPi site.  Downloading and Testing Your Package From Test PyPi It’s now time to run a few quality checks on your global package. To download and test your package from the Test PyPi repository, follow the next steps:\n  In the console, go to the root folder that contains your “mytestenvironment” virtual environment. Ensure you’re outside the “mytestenvironment” folder.\n  Activate your virtual environment using the following command:\n  source mytestenvironment/bin/activate Note that the prompt of your command line has changed. Now, It should be preceded by the name of your virtual environment as shown below:\n(mytestenvironment) [user]@[hostname]:[current_directory]$ With the virtual environment activated, install your package distribution using the following command and your package name:  pip install --index-url https://test.pypi.org/simple/ squarenum-package You’ll get the following output:\nCollecting squarenum-package Downloading https://test-files.pythonhosted.org/packages/c9/f0/387fd3f3edeb693d24bf191191d444ed87709463484d6cc7340896555515/squarenum_package-0.1.tar.gz Installing collected packages: squarenum-package Running setup.py install for squarenum-package ... done Successfully installed squarenum-package-0.1 Let’s now import our brand new Python package. In the console, open a Python interpreter:  python Import your global package distribution by running the following command:  from squarenum_package import squarenum “squarenum” is the Python file that contains your function logic and “squarenum_package” is your package name.\nFinally, test your function using the following command:  squarenum.calculate_square(7) 49 Note we used the dot notation to call the “calculate_square()” function inside the “squarenum” file. The dot notation means that you’re referring to something inside a file.\nPublishing Your Package to the PyPi Repository If you’ve made it this far, then you have already created, published, downloaded, and tested your global package distribution. Our starting point was to check whether our package behaved as expected when completing the task of publishing it to the Test PyPi repository. If everything went OK, our final step will be to publish it to the PyPi repository. Let’s find out how our package gets ready for production.\n In your console, go to ./squarenum_distribution_PyPi_test and upload your package distribution to PyPi by running the following commands:  cd /squarenum_distribution_PyPi_test twine upload dist/* When prompted, enter your PyPy account username and password. Recall that creating a PyPi account was a prerequisite. This is the right time to create one if you have not already done so.  Uploading distributions to https://upload.pypi.org/legacy/ Enter your username: Enter your password: In your browser, log in to your PyPi account. Finally, go to your projects section and check that your global package distribution has been uploaded.  Important:\nIf your global package distribution name contains an underscore symbol, this is changed into hyphen when it is uploaded to the PyPi site.  If your global package distribution name contains an underscore symbol, this is changed into hyphen when it is uploaded to the Test PyPi site.\nDownloading and Testing Your Package From PyPi This is the last step of our journey. To download and test your package from the PyPi repository, follow these steps:\n  In the console, go to the root folder that contains your “mypypi_environment” virtual environment. Ensure you are outside the “mypypi_environment” folder.\n  Activate your virtual environment using the following command:\n  source mypypi_environment/bin/activate Note that the prompt of your command line has changed. Now, It should be preceded by the name of your virtual environment as shown below:\n(mypypi_environment) [user]@[hostname]:[current_directory]$ With the virtual environment activated, install your global package distribution using the following command and your package name:  pip install squarenum-package You’ll get the following output:\nCollecting squarenum-package Downloading https://files.pythonhosted.org/packages/c9/f0/387fd3f3edeb693d24bf191191d444ed87709463484d6cc7340896555515/squarenum_package-0.1.tar.gz Installing collected packages: squarenum-package Running setup.py install for squarenum-package ... done Successfully installed squarenum-package-0.1 Now, let’s import our brand new Python package. In the console, open a Python interpreter:  python Import your global package distribution by running the following command:  from squarenum_package import squarenum “squarenum” is the Python file that contains your function logic and “squarenum_package” is your package name.\nFinally, test your function using the following command:  squarenum.calculate_square(7) 49 Note that we used the dot notation to call the “calculate_square()” function inside the “squarenum” file. The dot notation means that you’re referring to something inside a file.\n Conclusion Python packages allow developers to save time by reusing scripts, functions, and objects that others have developed. Let’s recap what we learned so far in this article.\nFirst, we took baby steps to figure out how to structure and create a package’s directory for local installation. This task was the foundation to take our package a step further and publish it to the Test PyPi repository. We realized that the structure of the parent folder for a local compared to a global package slightly changed. This is because we needed to include files that represented our signature for licensing and handling instructions on how to use our package. Finally, we published our package for global distribution in the PyPi repository.\nHere are a few takeaways about Python packages:\n Remember that once you’ve published a package, you are responsible for keeping it up-to-date and addressing any security issues. Python packages are versatile: you can pack not only functions but also objects, which is useful for OOP-based applications. You can learn more about Python packages by exploring existing packages. Have a look at the structure of packages like Numpy and Pandas.  "});index.add({'id':11,'href':'/posts/season-docs-julia/','title':"Julia Documentation Project",'content':"This is a sample document that illustrates how I audited, developed, and presented a proposal for the Julia Language project for the Google Season of Docs 2021 edition\nThe Julia Language Proposal for Season of Docs 2021 I would like to express my interest in participating in the project named Create a Comprehensive Julia Contributing Guide that is taking part in the Google Season of Docs 2021. Currently, I am looking for opportunities to contribute and sum efforts to the open-source software (OSS) community. I think we can be a good fit because of the following reasons.\nI am a data enthusiast. Six months ago, I started improving my Python programming skills and gaining more knowledge about data analytics through the Data Engineering Program from Udacity.\nA few months ago, I worked with a development team of 15 engineers to define a documentation strategy. This strategy followed a docs-like-code approach where the engineers had specific guidelines to contribute to documentation fostering consistency and collaboration. You can have a look at some of the procedures we followed in this README.\nAs part of my daily role, I am familiar with gathering information from subject matter experts (SMEs) ranging from end-users and stakeholders to developers, data engineers, and solution architects. This experience will be valuable when collecting information from users of the Julia ecosystem. Besides, I have over ten years of experience in Academia, where teaching, mentoring undergraduate students, and publishing journals were some tasks I carried on. This experience can be beneficial to identify the primary audiences of the Julia community and define the most suitable learning pathways to drive them into action. If we understand Julia´s personas, it would be easier to set the right tone and voice for providing instructions.\nMy proposal consists of three main stages. I provide my estimates in terms of effort (strong, medium, small). Once I have a better understanding of the content, credentials, and dependencies, we can all together establish a timeframe.\n Gather feedback (medium) Julia’s contributor guide is for the community, and therefore they can provide us the best insights to improve it. For this stage, I suggest conducting a survey introducing questions that can help us identify the background of the end-users such as students, professors, enthusiasts, or OSS developers. The aim is to understand how they start working on a project and how they interact with the community and maintainers. Identify the primary audiences and define the learning pathways (strong) In this stage, I will perform analytics to gather insights from the collected data. Based on the identified audiences, I will define their learning pathways and structure different sections in the contributor guide focused on each audience. I will reuse the existing material from Julia’s contributors, as is the PR guide. Define the best approach to measure the project’s success (medium) This stage consists of helping define the best strategy to measure success in terms of page views, number of PRs, or metrics based on mentions of the contributor guide.  Looking forward to contributing to the Julia language.\nSalomon Marquez\nPortfolio | LinkedIn | GitHub | Scholar\n"});index.add({'id':12,'href':'/posts/season-docs-sympy/','title':"Sympy Documentation Project",'content':"This is a sample document that illustrates how I audited, developed, and presented a proposal for the Sympy project for the Google Season of Docs 2021 edition\nSymPy Proposal for Season of Docs 2021 I want to express my interest in participating in the project named Documentation Organization - SymPy that is taking part in the Google Season of Docs 2021. Currently, I am looking for opportunities to contribute and sum up efforts to the open-source software (OSS) community. I think we can be a good fit because of the following reasons.\nA few months ago, I worked with a development team of 15 engineers to define a documentation strategy. This strategy followed a docs-like-code approach where the engineers had specific guidelines to contribute to documentation fostering organization and consistency. Please refer to this architecture guide as a sample document created from this strategy. Besides, in this project, I gained experience working with static websites powered by Hugo.\nAs part of my daily role, I am familiar with gathering information from subject matter experts (SMEs) ranging from end-users and stakeholders to developers, data engineers, and solution architects. This experience will be valuable when collecting information from users of the SymPy site to find areas of improvement.\nMy proposal consists of five main stages. I provide my estimates in terms of effort (strong, medium, small). Once I have a better understanding of the content, credentials, and dependencies, we can all together establish a timeframe.\n Audit all sources of SimPy documentation (strong) Set an organization strategy for documentation (medium) In this stage, I will assess Diátaxis as a documentation system to organize SimPy content into four categories: How-To guides, Tutorials, References, and Explanations. Alternatively, I will evaluate a tagging system used in mkdocs that shows more scalability, particularly when searching documents that can fit into two or more categories. It is important that the proposed organization system is flexible to include new content such as the user guides. Classify documents based on the chosen organization strategy (strong) In this stage, I will organize content into categories complying with the organization strategy. Improve the SymPy site user experience (strong) In this stage, I will perform a survey introducing questions that can help us understand how end-users navigate, access, and consult the different types of documents of the SymPy site. Then, I will perform analytics to gather insights from the collected data and apply those recommendations to improve the SymPy site. To avoid content disruption of the original SymPy site, I recommend working on a development branch until deploying the last version. If time allows, I will explore if Sphinx can include Google Analytics to track the usability of the SimPy site. Test Sympy site (medium) In this stage, I will perform a final round of tests to assess the final version of the SymPy site. I recommend conducting another survey six months after the last release of the site to collect and compare valuable insights about its usability.  I am looking forward to contributing to SymPy. Please refer to my resume and portfolio for a more detailed description of my technical writing experience.\nSalomon Marquez\nPortfolio | LinkedIn | GitHub | Scholar\n 1. What ideas do you have for improving the SymPy documentation organization? and What sorts of challenges do you anticipate for this project and how will you work to overcome them? My proposal consists of five main stages. I provide my estimates in terms of effort (strong, medium, small). Once I have a better understanding of the content, credentials, and dependencies, we can all together establish a timeframe.\n  Set an organization strategy for documentation (medium effort)\n Ideas  Define the model to organize SymPy documentation. Create a documentation catalog.   Challenges  Discuss with mentors the best approach to organize information and define the guidelines so that the SymPy community can contribute to the documentation following this approach.   Proposed solutions  Analyze use cases of existing documentation following the Diátaxis organization system. For example, Numpy and Edo. Compare the Diátaxis system against other strategies that use a documentation catalog.      Audit all sources of SymPy documentation (strong effort)\n Ideas  Audit the four primary sources of information: SymPy Website, SymPy Documentation, SymPy Source Code, SymPy Wiki.   Challenges  Analyze SymPy documentation that includes over 243 .rst files and 576 Wiki pages.   Proposed solutions  Allocate more time to accomplish this stage of the project.      Classify documents based on the chosen organization strategy (medium effort)\n Ideas  Tag documents based on the organization model. Evaluate a tagging system used in mkdocs that shows more scalability, particularly when organizing documents that can fit into two or more categories.   Challenges  Test if Sphinx is compatible with a file tagging system.   Proposed solutions  Instead of tags, organize the information using folders in the SymPy doc directory. Work on a proof of concept in the Test environment of the SymPy site to implement the file tagging system.      Improve the SymPy site user experience (strong effort)\n Ideas  Perform a survey that can help us understand how end-users interact with the SymPy site. Gather valuable insights from collected data. Update the SymPy site to include the new documentation categories and apply the collected feedback to leverage the accessibility and usability of the site. Explore if Sphinx can include Google Analytics to track the usability of the SymPy site.   Challenges  Foster the participation of the SymPy community to answer the survey. Update the SymPy site integrating newly organized content and recommendations from the community.   Proposed solutions  Launch the survey using Google Forms because the community is already familiar with using Google groups. Set up the new Test Environment for the SymPy site at the early stages of the project.      Test SymPy site (medium effort)\n Ideas  Test the new SymPy site for readiness, accessibility, and usability. Launch to production the new site.   Challenges  No challenges identified       2. How would you interact with the SymPy community during Season of Docs? I have different points of contact I can use to interact with the SymPy community.\n I will become part of the community. As an end-user and contributor to SymPy, I will understand the mechanisms of interaction and logistics underneath making a PR, for example. My approach to a broader audience, such as the SymPy community, will be through the [SymPy Google group](SymPy Google group). Alternatively, I can use other communication channels such as Gitter. I will plan to have weekly sessions with mentors of the SymPy project to provide updates and check progress. I will suggest using an alternative communication channel such as Hangouts or Slack to keep constant communication with mentors. Communication should be straightforward because there is only one hour difference between our time zones.   3. Finally, we would like to ask if you would prefer to work 10 hours or 20 hours a week for this project. I can commit to an average of 10 to 15 hours per week to achieve the goals of this project.\n"});index.add({'id':13,'href':'/posts/architecture-guide/','title':"Software Architecture Guide",'content':"The purpose of this guide is to describe the software architecture of the Betelgeuse application and its interaction with third-party applications. This document is useful to the developers and stakeholders of Orion who are implementing or updating new features for the Betelgeuse application.\n Introduction Betelgeuse is a serverless application hosted in the AWS cloud. Unlike the monolithic architecture of the previous Betelgeuse system, Betelgeuse is implemented as a containerized .NET application following a microservice-based architectural style. This style decomposes the monolithic approach into three main components, each associated with a microservice: bellatrix, rigel, and saiph. Each microservice undertakes a single business responsibility from the old Betelgeuse system as follows:\n Bellatrix Microservice: Handles transactions of individuals such as create/update clients and group leaders. It also manages brands, regions, routes, and groups. Rigel Microservice: Handles transactions related to credits and payments. Saiph Microservice: Handles the communication of Betelgeuse with third-party systems and processes transaction reports.  Although microservices run independently and, therefore, are built separately, they continuously interact among themselves. The microservices are built as container images orchestrated by the AWS Elastic Container Service (ECS). AWS Fargate provisions the infrastructure of the ECS clusters. Each microservice has its own database decoupled from the other microservices. Data is migrated from the previous Betelgeuse system and stored in serverless Aurora MySQL relational databases. Moreover, communication among the microservices is asynchronous and based on integration events. It follows the principles of request-response messaging for publish-subscribe channels, using both Amazon SNS topics and Amazon SQS queues.\nBetelgeuse is deployed on the cloud using the Octopus Deploy tools. GitLab CI/CD tools ensure continuous integration and continuous delivery of the application by implementing automated pipelines. In addition, Betelgeuse uses Terraform templates to implement Infrastructure as Code on AWS.\nFinally, the Betelgeuse client application is stored in a bucket of Amazon S3 and served as a Single Page Application (SPA) using AWS Cloudfront. The components of the client application are built using the Angular Framework.\nThe Betelgeuse application comprises the following modules:\n Betelgeuse Identity and Authorization Module Betelgeuse Web Frontend Betelgeuse Backend Betelgeuse Security Betelgeuse Monitoring  Figure 1 provides a high-level description of the Betelgeuse application.\nFigure 1. Betelgeuse Architecture\nThe architectural style of Betelgeuse follows the good practices of Clean Architecture. This style is suitable for complex business logics that involve several areas. The advantages of the Betelgeuse architecture are the following:\n It follows the Dependency Inversion Principle (DIP). This principle states that high-level modules should not depend on low-level modules. The interplay between both modules must be thorugh abstractions. Therefore, high-level modules of the Betelgeuse such as data, infrastructure, dependencies, and user interface are decoupled from low-level modules (business rules). This principle focuses the Betelgeuse architecture on the business logic of Orion. It complies with a Domain Driven Design (DDD). DDD is a software development approach that links the implementation of the application to the core business concepts of the client. This approach enables to map the old monolithic architecture of Betelgeuse consisting of use cases into domains. Each domain then correlates to a microservice in the new model of Betelgeuse. It is scalable. Microservices can scale up as Orion business logic does. Each microservice can assume a single business responsibility.   Betelgeuse Identity and Authorization Module This section describes the business logic of Betelgeuse for an end user to access and navigate the application according to their role in Orion.\nAmazon Cognito is the AWS service that enables authentication and authorization of Betelgeuse end users. Cognito user pools verify the identity of users so that they can directly sign in and sign up to the application. A user pool enables Betelgeuse to create and maintain an end-user directory that includes information such as profile, company, zone, and office.\nBetelgeuse, includes two methods to authenticate an end user:\n Directly registering the new user’s profile in the AWS Cognito console. Using a third-party SAML 2.0 identity provider, such as Azure Active Directory, for integrating Office 365 single sign-on (SSO), as Figure 2 shows.  Figure 2. Authentication Flow of Betelgeuse Users using Office 365\nThe user authentication flow using a third-party identity provider is the following:\n The user enters the domain of the Betelgeuse application. The application redirects the user to the Amazon Cognito hosted UI to authenticate using Office 365 SSO. Amazon Cognito redirects the user to the Microsoft login site to enter their Office 365 credentials. If the user authenticates successfully, Azure Active Directory provides four claims related to the user profile: role, office, zone, and company to Amazon Cognito using the SAML 2.0 standard, an industry standard for federated authentication.  Note:\nOriginally, Orion’s user information was kept on a local Windows server. Eventually, the user directory was uploaded to the Azure Active Directory in the cloud using the Azure AD connect service.  Next, Cognito issues a JSON Web Token (JWT) to the Betelgeuse application using the OAuth 2.0 implicit grant flow, which enables Cognito to directly return a JWT to Betelgeuse after the user successfully authenticates. The OAuth 2.0 standard is used to control user authorization. The Amplify framework decodes and verifies the JWT. If valid, it grants the user access to the application.  Important:\nThe Betelgeuse application uses the SDKs and libraries provided by the AWS Amplify framework to integrate AWS services. Amplify also handles the Betelgeuse sign in and sign up flow by implementing direct calls to the methods of the Auth class using the SDK of Angular CLI.   Betelgeuse Frontend This section describes the elements, along with their properties and relationships, that support the client and static content of Betelgeuse. An end user interacts with these elements after successfully identifying and accessing the application.\nThe Betelgeuse application uses the Angular JavaScript framework to build a Single-Page web Application (SPA). Amazon CloudFront serves the client and static content of the application, stored in an S3 Bucket. Betelgeuse Frontend communicates to the backend microservices rigel, bellatrix, and saiph using a REST API Interface.\nA CloudFront distribution is a reliable and secure way to serve content more efficiently because it enables Betelgeuse to cache the most frequently requested content. Each CloudFront distribution uses anSSL certificate to identify the Betelgeuse site and secure its private network communications. Moreover, each deployment environment: development, laboratory, testing, and production has its own CloudFront distribution.\nFrontend DNS (Domain Name System) requests are routed to the CloudFront distribution using the Amazon Route 53 service by means of an alternate DNS domain. The CloudFront distribution points to the origin S3 Bucket that hosts the client application. The S3 Bucket is configured for website hosting with policies to grant external access. For more details, check the section Routing Traffic to CloudFront Distributions.\n Betelgeuse Backend This section describes the following topics:\n The architecture of the microservices as containerized images along with their infrastructure provisioning The routing of incoming traffic from the Frontend The structure of the Virtual Private Cloud (VPC) and subnets The request-response messaging among microservices The deployment of the Betelgeuse application   Containerized-Based Microservices Betelgeuse uses a microservice-based architecture. Each microservice is a containerized application that has its own database. Betelgeuse includes three main microservices: bellatrix, rigel, and saiph. These microservices exist in all the deployment environments of the application: development, laboratory, testing, and production.\nFigure 3. Interplay between the ECS Cluster and AWS Fargate for Building the Microservices.\nAs Figure 3 shows, each deployment environment is built on an AWS ECS (Elastic Container Service) cluster and launched using AWS Fargate. Amazon ECS is an orchestration service that handles your containers using services. Services define tasks based on a task definition, which works as a blueprint that describes how to provision your containers. Tasks are one-time executions of your containers. For example, the development environment includes an ECS cluster with three ECS services:\n orion-dev-bellatrix orion-dev-rigel orion-dev-saiph  Each service can contain a single or multiple tasks representing a container image of a specific microservice. You can scale up to any number of tasks for maintaining your application’s availability if a task failure occurs.\nAs mentioned above, the ECS service builds a container image of a microservice using a task definition. A task definition is required to run Docker images in ECS. The task definition defines some parameters such as the origin Docker image, CPU, and memory for each task, launch type, and ports. Betelgeuse uses AWS ECR (Elastic Container Registry) to store the Docker images implemented and built in GitLab by the developers. The ECS service consumes the Docker images from the AWS ECR and builds the container image of the microservices using the task definition. For more details, consult the Deployment section. To ensure compatibility and usage of the latest image version, GitLab uses semantic versioning (semver) of images.\nAWS Fargate enables you to run your containers without requiring the whole capacity of EC2 instances. Fargate handles the infrastructure required for each ECS cluster. Fargate launch type provisions your tasks with the required number of CPU cores and gigabytes of memory defined in the task definition.\n Traffic Distribution Betelgeuse’s domain name is orion.com.mx. This domain includes the following subdomains:\n dev.orion.com.mx lab.orion.com.mx test.orion.com.mx api-dev.orion.com.mx api-lab.orion.com.mx api-test.orion.com.mx  Amazon Route 53 is responsible for routing traffic to orion.com.mx and its corresponding subdomains by means of a public hosted zone. A hosted zone contains records that define how the internet traffic is routed for Betelgeuse DNS queries. Amazon Route 53 handles DNS queries and routes traffic to a specific destination of the Betelgeuse application.\nThe Amazon Route 53 service main responsibilities include:\n Mapping domain names to CloudFront distributions, Application Load Balancers and S3 Buckets. Routing traffic for a domain and its subdomains using records. Determining how to respond to DNS queries using a routing policy. Determining the format of the value that returns in response to a DNS query.  The following sections describe in detail how Amazon Route 53 maps domain names to CloudFront distributions and Application Load Balancers.\n Routing Traffic to CloudFront Distributions To describe how Amazon Route 53 distributes traffic to a CloudFront distribution, the following steps use the dev.orion.com.mx subdomain as an example.\n The frontend sends a dev.orion.com.mx DNS query. The Amazon Route 53 service routes traffic for this subdomain to the alternate domain name CNAME associated with the CloudFront distribution.  Note:\nThe DNS query name must exactly match the CloudFront alternate domain name. For example, if the alias record name is dev.orion.com.mx the alternate domain of the CloudFront distribution must be named likewise.  The CloudFront distribution points to the S3 bucket endpoint and serves the client and static content stored in the bucket.  Note:\nThe CloudFront distribution uses an SSL certificate to secure all communications among the AWS resources of the internal network. An SSL certificate is created per domain name using the Amazon ACM service.   Routing Traffic to Application Load Balancers Betelgeuse microservices are placed behind an HTTP/HTTPS load balancer secured by an SSL certificate. The load balancer routes traffic to the microservices based upon inbound rules defined on specific ports. Furthermore, the application load balancers have a security group, which acts as a virtual firewall to control inbound and outbound traffic.\nTo describe how Amazon Route 53 distributes traffic to an application load balancer, the following steps use the api-lab.orion.com.mx subdomain as an example.\n The frontend sends a api-lab.orion.com.mx/bellatrix DNS query. Amazon Route 53 routes all incoming traffic from this DNS name to the application load balancer (ALB) of the laboratory environment. The ALB redirects traffic to a microservice target group based on the route name (/bellatrix/, /rigel/, or /saiph/).  Important:\nThe ALB is configured to listen to ports: 80, 443, 8080, 8443, 8810, and 8811. ALB listeners check for connection requests that use the HTTP or HTTPs protocols on these ports. Each listener includes rules to route incoming traffic to a specific target group.  The target group contains a target of type IP. In this case, the target is the private IP address assigned to the task of the bellatrix microservice. This IP address is dynamic and changes every time the task or the service is restored.  Note:\nAll incoming traffic from ports that use the HTTP protocol is redirected to ports using the HTTPs protocol. This configuration ensures that all incoming requests have an SSL certificate to secure the connection.  For security purposes, each deployment environment has its own Virtual Private Network (VPC) that complies with the network architecture of Orion. A VPC contains three availability zones. Each availability zone includes a private and an isolated subnet for a total of six subnets. Private subnets host container images of the microservices. On the other hand, isolated subnets host databases. Besides, the ALB has an associated SSL certificate to secure communications and encrypt sent data. The Betelgeuse application has an additional security layer, the user must connect to Orion’s VPN to access the microservices resources.\nNote:\nOnly the development environment has both the microservice ECS tasks and databases configured on private subnets. Isolated subnets are not used.   Asynchronous Communication Communication among the microservices that integrate the Betelgeuse application is based on events using both Amazon Simple Notification Service (SNS) topics and Amazon Simple Queue (SQS) service. This strategy ensures a decoupled and asynchronous interaction among the different microservices that follows the principle of request-response messaging for publish-subscribe channels. In this scheme, an event bus collects all the events produced by a microservice. Then, it distributes them among the subscribed queues of other microservices, as Figure 4 shows.\nAs an example, the following steps describe in detail the communication scheme of the bellatrix microservice with the rigel and saiph microservices.\n The bellatrix microservice produces a new event. The Amazon SNS service, that listens to events from the list of topics of all the microservices, performs two actions:  It fans this event out to the list of registered topics of the rigel and saiph microservices. It processes all event logs parallelly and stores them in an S3 bucket for further reference and inspection. To accomplish that,  A Lambda function pushes the events to Amazon Kinesis Data Firehose. The Kinesis Data Firehose service captures and delivers the streaming data (events) into an S3 bucket.\nIf required, a second Lambda function can perform an identity transformation of the events in the stream.     The Amazon SQS service subscribed to the topic collects the new event. The Amazon SQS service appends the event to the queue of the rigel and saiph microservices, respectively. The rigel and saiph microservices process the event. Parallelly, the event is collected by a Death Letter Queue (DLQ) to ensure that the event is processed. A DLQ can handle duplicated events. If required, all DLQ logs can be processed by a Lambda function and stored in an S3 bucket.  Figure 4. Asynchronous and Decoupled Communication among Betelgeuse Microservices.\n Deployment Betelgeuse uses the Octopus tools for deploying the containerized images of the microservices in the cloud. Each deployment environment (development, testing, laboratory, and production) includes five containerized images. Octopus executes a PowerShell script that tells the AWS CLI which processes and resources to implement, as well as which specific version of the images stored in the AWS ECR to use. Figure 5 describes the deployment process of the Betelgeuse application.\nFigure 5. Deployment Process of Betelgeuse Environments using Octopus\nTo understand the deployment process of containerized images, the following steps describe how Octopus deploys the bellatrix image into the development environment.\n A developer makes a pull request to the Orion.Betelgeuse.Bellatrix repository. GitLab CI/CD implements automatically the following stages:  Prebuild  Determines image version using semantic versioning specification v.1.0.0 (SemVer).   Build and Test  Executes unit tests to the .NET project and all its dependencies. Builds, analyzes static code, and publishes the .NET core application and its dependencies to a folder for deployment.   Push  Builds, tags, and pushes a dockerized image of the .NET core application to the container registry of GitLab (CI_REGISTRY_IMAGE). Builds, tags, and pushes a dockerized image of the .NET core application to the AWS Elastic Container Registry (ECR). Packs the powershell deployment script into a NuGet package and pushes it to the Octopus built-in repository.   Deploy  Creates a release with the specified version in the Prebuild step and deploys it to the development environment.     Octopus runs the PowerShell script, which contains custom deployment actions to be performed by the AWS CLI.\nOctopus tells AWS ECR to deploy the bellatrix image that matches the specified version number of the created release.  Important:\nOctopus enables custom powershell scripts to have access to the AWS CLI. This requires a previous authentication step to provide AWS credentials to Octopus.   Betelgeuse Security The Betelgeuse application complies with the following security checkpoints:\n General Security Checkpoints  Users must connect to the Orion VPN to access the application. Users must have Office 365 credentials or have a username and password provided by the IT department to enter the application.\nThe IT department can create, organize, and update users directly on the AWS console of Cognito based on a role and permission matrix. Users have restricted permissions to particular Betelgeuse functionalities based on their role and the permission matrix.  Note:\nThe permission matrix applies to each endpoint of every microservice (bellatrix, rigel, and saiph) and to particular functionalities (windows) of the application’s frontend.   Architectural-Level Security Checkpoints  S3 buckets that store the client application have configured access policies. Cloudfront distributions that serve the client application have a SSL certificate. Application Load Balancers that distribute frontend DNS queries to the microservices have a SSL certificate. Each deployment environment has its own VPC including three private and three isolated subnets. Private subnets host the microservices whereas isolated subnets host the databases. Developers can connect to the isolated Amazon Aurora RDS databases from their local machine using an Amazon EC2 instance as a bastion host. This approach enables them to securely administer and give maintenance to the databases. Infrastructure as code of Betelgeuse uses Terraform templates. AWS Systems Manager Parameter Store manages secrets such as passwords, API keys, and other sensitive information (in a string format) that is consumed by the Terraform templates. All requests from the frontend to the backend using the REST API interface require a bearer token. All incoming traffic to the microservices is redirected by the Application Load Balancers to port 443 that uses the HTTPs protocol.   Betelgeuse Monitoring The AWS CloudWatch service monitors the AWS infrastructure of Betelgeuse. This service enables you to select metrics to audit AWS resources such as EC2 instances, logs, SNS messages, SQS queues, CloudFront distributions, Application Load Balancers, and so on. Additionally, Grafana is integrated in the technological stack of Betelgeuse as an open source analytics and monitoring solution to check the AWS infrastructure.\n"});index.add({'id':14,'href':'/posts/how-to-guide/','title':"Deploying Betelgeuse App using Octopus",'content':"This guide explains you how to manually deploy to the AWS Cloud any of the following Betelgeuse projects using Octopus deploy:\n Orion.Client Orion.Bellatrix Orion.Rigel Orion.Saiph  Note:\nThese projects are automatically deployed to the development and testing environments. Both environments share the same projects and versions. For the laboratory environment, some of these packages are deployed manually.  The deployment process of Betelgeuse projects is detailed in the Architecture Guide. Briefly, the process consists of four stages that run automatically after a developer makes a pull request in GitLab:\n Prebuild. Determines image version. Build and Test. Builds, analyzes static code, and publishes the .NET core application and its dependencies to a folder for deployment. Push. Builds, tags, and pushes a dockerized image of the .NET core application to the container registry of GitLab and to the AWS Elastic Container Registry (ECR). It also packs the powershell deployment script into a NuGet package and pushes it to the Octopus built-in repository. Deploy. Creates a release with the specified version in the Prebuild step and deploys it to a deployment environment.  This guide describes how you can run the Deployment stage manually.\n Log In to Octopus  Go to Octopus Deploy site. Log in to Octopus.  If you are a admin user:  In the Log in page, click on the Google button to sign in using admin SSO. Identify yourself using your admin credentials.   If you are a dev user:  Identify yourself using your dev credentials.      Once you login, you are redirected to the Octopus dashboard that displays all the Betelgeuse projects and their release versions on their respective deployment environments.\n Deploy Betelgeuse Projects Manually This section describes how you can deploy Betelgeuse projects manually using Octopus GUI. As an example, the following steps illustrate the process for deploying the bellatrix microservice manually.\nSelect a Project  In the Octopus dashboard screen, select the Orion.Bellatrix project under the Default Project Group column. The application shows an overview of the project and its versions deployed in each of the deployment environments.  Create a Release  Click on the CREATE RELEASE button, located on the top left corner of the screen under the project’s name.\nOctopus loads the latest version release included in the Octopus Built-in registry pushed from GitLab. It also includes all the packages integrated in this release. Select the version and packages for the release you want to deploy. Click on the SAVE button located at the top right corner of the section. On the left panel, click on Overview.\nOctopus displays the latest release that you created in step 1.  Deploy the Release  Click on the DEPLOY… button under the environment where you want to deploy the release.\nOctopus redirects you to a new screen that shows you step-by-step the progress of the task. Click on the TASK LOG tab to check the deployment logs. Filter the deployment logs with the following parameters:  Expand = All Log level = Verbose Log fail = All Important:\nDeployment logs can help you to monitor and debug the deployment process.     In the top menu, go to Dashboard to verify that the deployment process is ready for the deployment environment you selected.  "});index.add({'id':15,'href':'/posts/season-docs-intermine/','title':"Intermine Documentation Project",'content':"This is a sample document that illustrates how I audited, developed, and presented a proposal for the Intermine project for the Google Season of Docs 2020 edition\nWelcome to the InterMine Documentation Site InterMine is an open source data warehouse build specifically for the integration and analysis of complex biological data. In this site you can find documentation and tutorials to get you up to speed with our brand new platform BlueGenes.\n Quick Start Learn in 5 min BlueGenes  How-To Guides Know BlueGenes Features  Tutorials Learn a task in BlueGenes    Sample Video Tutorial   Want to Learn More?\nVisit our YouTube channel and have access to all our tutorials.   InterMine User Training Docs | Proposal Overview Provide a brief overview of the project. What problem are you going to solve and how are you going to solve it?\n\u0026ldquo;InterMine is an open source biological data warehouse, designed to help biologists analyse and query data\u0026rdquo;. Currently, the InterMine platform is undergoing a migration process to a new one named BlueGenes. This new modern interface requires the creation of documentation to encourage new and existing end-users to use it. Based on an audit of the InterMine docs and existing resources, there are several opportunity areas for the new documentation of BlueGenes in terms of accessibility, organization, consistency, and updating.\nThe aim of this proposal is to fulfill the documentation requirements of BlueGenes, focusing on a non-technical and semi-technical audience, by following a DocOps strategy. DocOps is a methodology that treats documentation as code fostering a collaborative ecosystem to produce high-quality, centralized, and updated documentation.\nThe BlueGenes documentation will be delivered in a static site powered by Hugo using GitHub Pages as hosting provider. A central repository will store all documentation related to the site such as how-to guides, tutorials, and media resources. The resources will combine text, screenshots, and videos creating a seamless learning experience for the end-users. Click on this link, to consult an example of a documentation site and a video tutorial for BlueGenes.\nThe proposal includes the following sections:\n Statement of the problem. Includes an audit of the current InterMine documentation. Implementation plan. Describes the documentation strategy for InterMine. Timeline and Milestones. Details the step-by-step process for the implementation plan. Deliverables. Lists the documents that can be created. Supporting information. Includes background of the Technical Writer and references that support the proposal.   Statement of the Problem State clearly what the problem is. Please use the project requirements as a guide, but do not copy and paste.\nThis section describes the current state of the InterMine documentation. The following table lists the opportunity areas after auditing the different sources of documentation.\n   Opportunity Area Description     Up-to-date docs Since the creation of the documentation repository in 2014, scattered efforts for adding and/or updating content have been recorded. Particularly, the content for the tutorials section, which was created in 2016, has not been updated since then. The brand new BlueGenes platform requires updated documentation to increase its usage.   Well-ordered docs End-users find it difficult to access relevant information of the InterMine\u0026rsquo;s main features because the documentation is hosted in two different sites. Whereas the InterMine Documentation site consists of tutorials and exercises, the InterMine HomePage allocates tutorials in a video format along with other useful resources in Python. Documentation must exist in a single site having its own repository.   Consistent content The content of tutorials and exercises is composed of different resources such as text, screenshots, and videos. However, not all this content complies with the same template. InterMine documentation site can offer end-users a seamless learning experience. This can be achieved by making the learning resources consistent.   Collaborative docs Four people contribute to the InterMine documentation Rachel Lyne, Joshua Heimbach, Calvin Job Puram, and Yo Yehudi (maintainer). However, there are no clear and specific guidelines of how the community of InterMine users can contribute to documentation (a how-to guide to contribute). Besides, the tone, voice, and style of InterMine documentation is not defined. A style guide can foster community interaction with the documentation site to upload, renew, and fix content in a systematic way.   Platform compatibility The InterMine HomePage uses Hugo as a static generator whereas the InterMine Documentation site uses Sphinx. Both sites must use the same infrastructure to ease the creation, collaboration, and maintenance of the InterMine documentation site.   Resources compatibility Media resources are hosted on different locations such as YouTube and TechSmith screencast. Besides, the duration and format of the videos is not consistent. Some videos use tag description while others use a voice-over description format. By defining the infrastructure, specific guidelines, and a style guide, all resources can be consistent and user-friendly.     Implementation Plan Try to be concise, but do explain your plan thoroughly.\nThe implementation plan for this project consists of four stages:\n Agreements Stage Working Stage  Setting up Infrastructure Documenting the main features of BlueGenes Documenting the main Biology-related topics Documenting Tutorials that cover the main tasks that can be done with BlueGenes.   Closing Stage  Documenting a Product Overview of BlueGenes Documenting InterMine\u0026rsquo;s Contributors Guide (optional)   Final Stage  Creating Project\u0026rsquo;s Report Completing Mentor\u0026rsquo;s Assessments    Agreements Stage\nThis stage refines the main goals, tasks, and deliverables of this proposal taking into account the documentation risks. I will start reviewing the opportunity areas for the InterMine documentation with the team. My interest is to foster a very close collaboration with the team and members of the community and identify all sources of information to document. Together, we will settle down the foundations of the project\u0026rsquo;s infrastructure to build and create the docs.\nWorking Stage\nThis stage focuses on setting up the infrastructure of the documentation site and documenting the main features of BlueGenes. I will explore and use the BlueGenes platform by myself with the fresh eyes of an end-user. This will give me the freedom to start documenting the main features that are already on the production stage. Fig. 1 shows the documentation lifecycle that I will follow.\nFig. 1 Documentation Lifecycle for InterMine.\nClosing Stage\nThis stage focuses on the creation of a product overview of BlueGenes. Having created all the content of the site, I will produce a getting started section where end-users can get up to speed with the tools that BlueGenes offers. This can be a five-minute tutorial section. As a final deliverable, I will give a demo session to the InterMine\u0026rsquo;s community showing the documentation generated in the static site. If time allows, this stage includes the creation of a How-to guide for contributors.\nFinal Stage\nThis stage includes all paperwork related to the project´s closure.\n Timeline: Milestones Cover September to December 2020 milestones.\nThis section describes the milestones for the project based on the implementation plan.\nCommunity bonding\t(August 17, 2020 - September 13, 2020)\nDuring this stage of the project, I suggest the following activities:\n Get to know InterMine\u0026rsquo;s mentors to foster team collaboration. Refine goals, tasks, and deliverables of this proposal. Run an investigation period to define:  InterMine\u0026rsquo;s documentation infrastructure Style and tone for the documentation Media style and format    Doc development (September 14, 2020 - November 30, 2020)\nThe following table describes the milestones and timeline for this proposal.\n  Deliverables and Tasks      1. Setting up documentation infrastructure [1 week]      Static site infrastructure      Hosting infrastructure for static content such as videos      CI/CD tools (if required)    2. Documenting BlueGenes main features [4 weeks]      How-to guide for MyMind Account      How-to guides for searching tools        Keyword Search        Template Search        Query Builder        Region Search        List Search      How-to guides for viewing data tools        Report Pages        List Analysis Pages        Result Tables        Region Search Results        List Analysis Pages    3. Documenting How-to guides for Biology-related content [4 weeks]      Gene Ontology      Expression Data      Pathways      Regulatory Data      Orthologues      Interactions    4. Documenting Tutorials [3 weeks]    5. Documenting BlueGene’s Product Overview [1 week]    6. Documenting a How-to guide to contribute to InterMine’s documentation (optional) [1 week]      Define a Style Guide      Create README for the main repository      Integrate VALE as a tool to validate documentation’s style and tone    7. Work on Final Report [1 week]    8. Complete Mentor’s evaluations [1 week]     Deliverables Indicate which deliverables are required and which are optional. Include presentation of your project on the InterMine community call as a deliverable.\nThis section lists some of the deliverables included for this proposal.\n A static site powered by Hugo and hosted in GitHub Pages. How-to guides for the main features of BlueGenes. How-to guides for the main Biology-related topics. Tutorials that cover the main tasks that can be done with BlueGenes. A Product Overview of BlueGenes. A how-to guide to contribute to InterMine\u0026rsquo;s documentation (optional). A Style Guide for InterMine\u0026rsquo;s documentation (optional). VALE as a tool to check the fundamentals of the style guide (optional).   Documentation Risks Assumptions\nThe technical writer assumes the following:\n At least one stakeholder from InterMine must review and approve all deliverable documents. Documentation is delivered in a static site generator.  Dependencies and Risks\nThe InterMine documentation has the following dependencies and risks:\n Documentation delivery estimates are subject to change based on the scope of the project. Late changes of the scope and requirements can prevent timely delivery of high-quality documentation. Late feedback for documentation in the reviewing process can affect the high-quality and timely delivery of the documentation.  "});index.add({'id':16,'href':'/posts/api-guide/','title':"API Documentation",'content':"This document explains the implementation of BlueSnap payment processor with SampleClient web services. It describes the payment logic, provides use cases, and shows the payment sequence diagrams for each case. The target audience of this document includes stakeholders, developers, and project managers from SampleClient and MyEnterprise.\n Overview SampleClient requires to integrate BlueSnap as an alternative payment processor for its web services. SampleClient payment processors include:\n Stripe Humboldt Paysafe Safecharge  The first stage of BlueSnap implementation with SampleClient enables payments using a credit card. This document describes the payment flow of a New Vaulted Shopper Making a Purchase.\n A New Vaulted Shopper Making a Purchase The scenario for this purchase case is the following:\n A new shopper signs up for the first-time to the SampleClient web services. The shopper decides to buy credits to have access to SampleClient content using a credit card. The shopper selects the number of credits and proceeds to the checkout form. The SampleClient payment logic selects BlueSnap as payment processor from among several processors (Stripe, Humboldt, Paysafe, and Safecharge) to make a purchase with a credit card. The shopper fills up all required fields to complete the transaction. The payment processor asks permission to store the credit card information for future transactions. The shopper clicks the Buy securely now button. The shopper receives a notification status about the purchase transaction.   Sequence Diagram The following diagram shows the payment logic for a shopper making a purchase using BlueSnap payment processor.\nFigure 1. Payment Sequence Diagram for a Shopper Making a Purchase using BlueSnap Processor\nThe process of the payment sequence for a shopper making a purchase is the following:\n The frontend sends a GET request to the backend to load the BlueSnap payment form with the Hosted Payment Fields. The backend sends a POST request to the BlueSnap API to retrieve a Hosted Payment Fields token (pfToken). The BlueSnap API returns the pfToken on the Location header. The backend returns to the frontend the following parameters: pfToken, base_url, and billing_workflow. The frontend loads the checkout form with BlueSnap Hosted Payment Fields. The shopper clicks the Buy securely now button after filling out all of the fields. The frontend sends three PUT requests to the BlueSnap API to protect customer-sensitive data (CCN, Expiration date, and CVV). The frontend receives a confirmation of the binding between the pfToken and shopper’s card information. The frontend sends a POST request to backend to make a purchase using the pfToken and shopper’s payment details. The backend sends a POST request to BlueSnap API to vault a new shopper. The backend receives a BlueSnap Customer ID associated with the new vaulted shopper. The backend sends a POST request to BlueSnap API to make a purchase using the BlueSnap Customer ID. The backend receives a notification of the payment status. The backend sends back the notification of the payment status to the frontend. The frontend displays a window with the payment status.   Endpoints The following table lists the endpoints involved in the payment logic described above:\n  From   To   Endpoint     Frontend    Backend    GET /v2/billing/creditcard      Backend    BlueSnap API    POST /payment-fields-tokens      Frontend    BlueSnap API    PUT /payment-fields-tokens/      Frontend    Backend    POST /v2/billing/creditcard      Backend    BlueSnap API    POST /vaulted-shoppers      Backend   BlueSnap API    POST /transactions    GET /v2/billing/creditcard This request enables the frontend to receive a transaction token and load the BlueSnap payment form.\nURL http://api-dev.sampleclient.com/v2/billing/creditcard/?is_mobile=1 \\ Header Parameters The following table lists the header parameters used in the request.\n  Parameter  Description  Type  Required / Optional    Authorization  Specifies the bearer token for SampleClient API authentication. Value: Bearer [USER_TOKEN].  string  required    Content-Type  Specifies the format of the content response. Value: application/x-www-form-urlencoded.  string  required    cache-control  Indicates whether there is a cache control. Value: cache and no-cache.  string  required    Query Parameters The following table lists the query parameters used in the request.\n  Parameter  Description  Type  Required / Optional    is_mobile  Specifies whether the shopper is using a mobile device. If true, value: 1.  integer  required    packages  Specifies the number of credits the shopper is buying. Value: 5, 11, 22.  integer  required    Sample Request The following is an example of a command-line request.\ncurl -X GET \\ \u0026#39;http://api-dev.sampleclient.com/v2/billing/creditcard/?is_mobile=1\u0026amp;packages=11\u0026#39; \\ -H \u0026#39;Authorization: Bearer [USER_TOKEN]\u0026#39; \\ -H \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; \\ -H \u0026#39;cache-control: no-cache\u0026#39; \\ Response Body The following table lists the elements commonly used from the response.\n  Element  Description  Type    bluesnap_pars  Contains information on the transaction token.  data object      base_url  Specifies the base API URLs for the BlueSnap environments. Values: sandbox and production.  string      hostedFieldToken  Specifies the generated transaction token.  long    rebuy  Contains information about the stored payment methods of a vaulted shopper.  array    Important:\nThe elements listed in the response body table are in blue color in the Sample Response Body.  Sample Response Body The following is an extract of a typical response showing some of the elements returned for this request.\n{ \u0026#34;creditcardform\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;options\u0026#34;: [], \u0026#34;multi\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;packages\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Name on Credit Card\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cc_fullname\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Street Address\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;street\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;City\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Beverly Hills\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;city\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Location \u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;225\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;id\u0026#34;: -1, \u0026#34;name\u0026#34;: \u0026#34;--------------\u0026#34; }, { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;AFGHANISTAN\u0026#34; }, ... { \u0026#34;id\u0026#34;: 239, \u0026#34;name\u0026#34;: \u0026#34;ZIMBABWE\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;country\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;ZIP/Postal Code\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;90210\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;postal_code\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;rebuy\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rebuy\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;MIC\u0026#34;, \u0026#34;value\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;fic_opt_in\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;MPlus\u0026#34;, \u0026#34;value\u0026#34;: false, \u0026#34;name\u0026#34;: \u0026#34;mplus_opt_in\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;boolean\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Check here to top up your credits when you run out so that you are ready for any encounter.\u0026#34;, \u0026#34;value\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;autobill_opt_in\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Credit Card number\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cc_number\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Credit Card Expiration Date\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;01\u0026#34; }, ... { \u0026#34;id\u0026#34;: 12, \u0026#34;name\u0026#34;: \u0026#34;12\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;cc_month\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;select\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Credit Card Expiration Date\u0026#34;, \u0026#34;options\u0026#34;: [ { \u0026#34;id\u0026#34;: 2019, \u0026#34;name\u0026#34;: \u0026#34;2019\u0026#34; }, ... { \u0026#34;id\u0026#34;: 2068, \u0026#34;name\u0026#34;: \u0026#34;2068\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;cc_year\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;label\u0026#34;: \u0026#34;Card Verification Code (CVV) #\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;cc_cvv\u0026#34; } ] } POST /payment-fields-tokens This method enables to create a Hosted Payment Fields token (pfToken) when using Hosted Payment Fields. It is a server-to-server request from SampleClient to BlueSnap when the customer access the checkout form. The token serves to protect sensitive shopper information during a transaction. The transactions using a pfToken are:\n Processing a purchase. Creating a vaulted shopper. Updating a vaulted shopper with a new credit card.  Important:\nThe Hosted Payment Fields token expires after 60 minutes.  URL https://sandbox.bluesnap.com/services/2/payment-fields-tokens Header Parameters The following table lists the header parameters used in the request.\n  Parameter  Description  Type  Required / Optional    Content-Type  Specifies that the format of the response body is a JSON object.  Value: application/json.   string  required    Accept  Specifies that the format of the accepted request is a JSON object.  Value: application/json.  string  required    Authorization  Specifies the bearer token for BlueSnap API authentication. Value: Bearer [USER_TOKEN]  string  required    Sample Request The following is an example of a command-line request.\ncurl -I -X POST \\ https://sandbox.bluesnap.com/services/2/payment-fields-tokens \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -H \u0026#39;Accept: application/json\u0026#39; \\ -H \u0026#39;Authorization: Bearer [USER_TOKEN]\u0026#39; \\ Sample Response Body The following is an example of a typical response that shows all the elements of the Hosted Payment Field token generation.\nHTTP/1.1 201 201 Date: Wed, 27 Nov 2019 17:24:47 GMT Set-Cookie: JSESSIONID=XXXXXXXX; Path=/services; Secure; HttpOnly Location: https://sandbox.bluesnap.com/services/2/payment-fields-tokens/XXXXXXXX_ Content-Length: 0 Strict-Transport-Security: max-age=31536000 ; includeSubDomains Set-Cookie: XXXXXXX; Path=/ Set-Cookie: XXXXXXX; path=/services Via: 1.1 sjc1-bit32 Hosted Payment Fields Token Errors The following table lists the errors returned from a request related to the Hosted Payment Field token.\n  Code  Name  Description    14040  EXPIRED_TOKEN  Token is expired    14041  TOKEN_NOT_FOUND  Token is not found    14042  NO_PAYMENT_DETAILS_LINKED_TO_TOKEN  Token is no associated with a payment method    "});index.add({'id':17,'href':'/docs/','title':"Docs",'content':""});index.add({'id':18,'href':'/docs/julia2021/','title':"Julia Proposal for Season of Docs 2021",'content':"The Julia Language Proposal for Season of Docs 2021 I would like to express my interest in participating in the project named Create a Comprehensive Julia Contributing Guide that is taking part in the Google Season of Docs 2021. Currently, I am looking for opportunities to contribute and sum efforts to the open-source software (OSS) community. I think we can be a good fit because of the following reasons.\nI am a data enthusiast. Six months ago, I started improving my Python programming skills and gaining more knowledge about data analytics through the Data Engineering Program from Udacity.\nA few months ago, I worked with a development team of 15 engineers to define a documentation strategy. This strategy followed a docs-like-code approach where the engineers had specific guidelines to contribute to documentation fostering consistency and collaboration. You can have a look at some of the procedures we followed in this README.\nAs part of my daily role, I am familiar with gathering information from subject matter experts (SMEs) ranging from end-users and stakeholders to developers, data engineers, and solution architects. This experience will be valuable when collecting information from users of the Julia ecosystem. Besides, I have over ten years of experience in Academia, where teaching, mentoring undergraduate students, and publishing journals were some tasks I carried on. This experience can be beneficial to identify the primary audiences of the Julia community and define the most suitable learning pathways to drive them into action. If we understand Julia´s personas, it would be easier to set the right tone and voice for providing instructions.\nMy proposal consists of three main stages. I provide my estimates in terms of effort (strong, medium, small). Once I have a better understanding of the content, credentials, and dependencies, we can all together establish a timeframe.\n Gather feedback (medium) Julia’s contributor guide is for the community, and therefore they can provide us the best insights to improve it. For this stage, I suggest conducting a survey introducing questions that can help us identify the background of the end-users such as students, professors, enthusiasts, or OSS developers. The aim is to understand how they start working on a project and how they interact with the community and maintainers. Identify the primary audiences and define the learning pathways (strong) In this stage, I will perform analytics to gather insights from the collected data. Based on the identified audiences, I will define their learning pathways and structure different sections in the contributor guide focused on each audience. I will reuse the existing material from Julia’s contributors, as is the PR guide. Define the best approach to measure the project’s success (medium) This stage consists of helping define the best strategy to measure success in terms of page views, number of PRs, or metrics based on mentions of the contributor guide.  Looking forward to contributing to the Julia language.\nSalomon Marquez\nPortfolio | LinkedIn | GitHub | Scholar\n"});index.add({'id':19,'href':'/docs/intermine2020/','title':"Proposal Season of Docs 2020",'content':"Welcome to the InterMine Documentation Site InterMine is an open source data warehouse build specifically for the integration and analysis of complex biological data. In this site you can find documentation and tutorials to get you up to speed with our brand new platform BlueGenes.\n Quick Start Learn in 5 min BlueGenes  How-To Guides Know BlueGenes Features  Tutorials Learn a task in BlueGenes    Sample Video Tutorial   Want to Learn More?\nVisit our YouTube channel and have access to all our tutorials.   InterMine User Training Docs | Proposal Overview Provide a brief overview of the project. What problem are you going to solve and how are you going to solve it?\n\u0026ldquo;InterMine is an open source biological data warehouse, designed to help biologists analyse and query data\u0026rdquo;. Currently, the InterMine platform is undergoing a migration process to a new one named BlueGenes. This new modern interface requires the creation of documentation to encourage new and existing end-users to use it. Based on an audit of the InterMine docs and existing resources, there are several opportunity areas for the new documentation of BlueGenes in terms of accessibility, organization, consistency, and updating.\nThe aim of this proposal is to fulfill the documentation requirements of BlueGenes, focusing on a non-technical and semi-technical audience, by following a DocOps strategy. DocOps is a methodology that treats documentation as code fostering a collaborative ecosystem to produce high-quality, centralized, and updated documentation.\nThe BlueGenes documentation will be delivered in a static site powered by Hugo using GitHub Pages as hosting provider. A central repository will store all documentation related to the site such as how-to guides, tutorials, and media resources. The resources will combine text, screenshots, and videos creating a seamless learning experience for the end-users. Click on this link, to consult an example of a documentation site and a video tutorial for BlueGenes.\nThe proposal includes the following sections:\n Statement of the problem. Includes an audit of the current InterMine documentation. Implementation plan. Describes the documentation strategy for InterMine. Timeline and Milestones. Details the step-by-step process for the implementation plan. Deliverables. Lists the documents that can be created. Supporting information. Includes background of the Technical Writer and references that support the proposal.   Statement of the Problem State clearly what the problem is. Please use the project requirements as a guide, but do not copy and paste.\nThis section describes the current state of the InterMine documentation. The following table lists the opportunity areas after auditing the different sources of documentation.\n   Opportunity Area Description     Up-to-date docs Since the creation of the documentation repository in 2014, scattered efforts for adding and/or updating content have been recorded. Particularly, the content for the tutorials section, which was created in 2016, has not been updated since then. The brand new BlueGenes platform requires updated documentation to increase its usage.   Well-ordered docs End-users find it difficult to access relevant information of the InterMine\u0026rsquo;s main features because the documentation is hosted in two different sites. Whereas the InterMine Documentation site consists of tutorials and exercises, the InterMine HomePage allocates tutorials in a video format along with other useful resources in Python. Documentation must exist in a single site having its own repository.   Consistent content The content of tutorials and exercises is composed of different resources such as text, screenshots, and videos. However, not all this content complies with the same template. InterMine documentation site can offer end-users a seamless learning experience. This can be achieved by making the learning resources consistent.   Collaborative docs Four people contribute to the InterMine documentation Rachel Lyne, Joshua Heimbach, Calvin Job Puram, and Yo Yehudi (maintainer). However, there are no clear and specific guidelines of how the community of InterMine users can contribute to documentation (a how-to guide to contribute). Besides, the tone, voice, and style of InterMine documentation is not defined. A style guide can foster community interaction with the documentation site to upload, renew, and fix content in a systematic way.   Platform compatibility The InterMine HomePage uses Hugo as a static generator whereas the InterMine Documentation site uses Sphinx. Both sites must use the same infrastructure to ease the creation, collaboration, and maintenance of the InterMine documentation site.   Resources compatibility Media resources are hosted on different locations such as YouTube and TechSmith screencast. Besides, the duration and format of the videos is not consistent. Some videos use tag description while others use a voice-over description format. By defining the infrastructure, specific guidelines, and a style guide, all resources can be consistent and user-friendly.     Implementation Plan Try to be concise, but do explain your plan thoroughly.\nThe implementation plan for this project consists of four stages:\n Agreements Stage Working Stage  Setting up Infrastructure Documenting the main features of BlueGenes Documenting the main Biology-related topics Documenting Tutorials that cover the main tasks that can be done with BlueGenes.   Closing Stage  Documenting a Product Overview of BlueGenes Documenting InterMine\u0026rsquo;s Contributors Guide (optional)   Final Stage  Creating Project\u0026rsquo;s Report Completing Mentor\u0026rsquo;s Assessments    Agreements Stage\nThis stage refines the main goals, tasks, and deliverables of this proposal taking into account the documentation risks. I will start reviewing the opportunity areas for the InterMine documentation with the team. My interest is to foster a very close collaboration with the team and members of the community and identify all sources of information to document. Together, we will settle down the foundations of the project\u0026rsquo;s infrastructure to build and create the docs.\nWorking Stage\nThis stage focuses on setting up the infrastructure of the documentation site and documenting the main features of BlueGenes. I will explore and use the BlueGenes platform by myself with the fresh eyes of an end-user. This will give me the freedom to start documenting the main features that are already on the production stage. Fig. 1 shows the documentation lifecycle that I will follow.\nFig. 1 Documentation Lifecycle for InterMine.\nClosing Stage\nThis stage focuses on the creation of a product overview of BlueGenes. Having created all the content of the site, I will produce a getting started section where end-users can get up to speed with the tools that BlueGenes offers. This can be a five-minute tutorial section. As a final deliverable, I will give a demo session to the InterMine\u0026rsquo;s community showing the documentation generated in the static site. If time allows, this stage includes the creation of a How-to guide for contributors.\nFinal Stage\nThis stage includes all paperwork related to the project´s closure.\n Timeline: Milestones Cover September to December 2020 milestones.\nThis section describes the milestones for the project based on the implementation plan.\nCommunity bonding\t(August 17, 2020 - September 13, 2020)\nDuring this stage of the project, I suggest the following activities:\n Get to know InterMine\u0026rsquo;s mentors to foster team collaboration. Refine goals, tasks, and deliverables of this proposal. Run an investigation period to define:  InterMine\u0026rsquo;s documentation infrastructure Style and tone for the documentation Media style and format    Doc development (September 14, 2020 - November 30, 2020)\nThe following table describes the milestones and timeline for this proposal.\n  Deliverables and Tasks      1. Setting up documentation infrastructure [1 week]      Static site infrastructure      Hosting infrastructure for static content such as videos      CI/CD tools (if required)    2. Documenting BlueGenes main features [4 weeks]      How-to guide for MyMind Account      How-to guides for searching tools        Keyword Search        Template Search        Query Builder        Region Search        List Search      How-to guides for viewing data tools        Report Pages        List Analysis Pages        Result Tables        Region Search Results        List Analysis Pages    3. Documenting How-to guides for Biology-related content [4 weeks]      Gene Ontology      Expression Data      Pathways      Regulatory Data      Orthologues      Interactions    4. Documenting Tutorials [3 weeks]    5. Documenting BlueGene’s Product Overview [1 week]    6. Documenting a How-to guide to contribute to InterMine’s documentation (optional) [1 week]      Define a Style Guide      Create README for the main repository      Integrate VALE as a tool to validate documentation’s style and tone    7. Work on Final Report [1 week]    8. Complete Mentor’s evaluations [1 week]     Deliverables Indicate which deliverables are required and which are optional. Include presentation of your project on the InterMine community call as a deliverable.\nThis section lists some of the deliverables included for this proposal.\n A static site powered by Hugo and hosted in GitHub Pages. How-to guides for the main features of BlueGenes. How-to guides for the main Biology-related topics. Tutorials that cover the main tasks that can be done with BlueGenes. A Product Overview of BlueGenes. A how-to guide to contribute to InterMine\u0026rsquo;s documentation (optional). A Style Guide for InterMine\u0026rsquo;s documentation (optional). VALE as a tool to check the fundamentals of the style guide (optional).   Documentation Risks Assumptions\nThe technical writer assumes the following:\n At least one stakeholder from InterMine must review and approve all deliverable documents. Documentation is delivered in a static site generator.  Dependencies and Risks\nThe InterMine documentation has the following dependencies and risks:\n Documentation delivery estimates are subject to change based on the scope of the project. Late changes of the scope and requirements can prevent timely delivery of high-quality documentation. Late feedback for documentation in the reviewing process can affect the high-quality and timely delivery of the documentation.  "});index.add({'id':20,'href':'/docs/sympy2021/','title':"SymPy Proposal for Season of Docs 2021",'content':"SymPy Proposal for Season of Docs 2021 I want to express my interest in participating in the project named Documentation Organization - SymPy that is taking part in the Google Season of Docs 2021. Currently, I am looking for opportunities to contribute and sum up efforts to the open-source software (OSS) community. I think we can be a good fit because of the following reasons.\nA few months ago, I worked with a development team of 15 engineers to define a documentation strategy. This strategy followed a docs-like-code approach where the engineers had specific guidelines to contribute to documentation fostering organization and consistency. Please refer to this architecture guide as a sample document created from this strategy. Besides, in this project, I gained experience working with static websites powered by Hugo.\nAs part of my daily role, I am familiar with gathering information from subject matter experts (SMEs) ranging from end-users and stakeholders to developers, data engineers, and solution architects. This experience will be valuable when collecting information from users of the SymPy site to find areas of improvement.\nMy proposal consists of five main stages. I provide my estimates in terms of effort (strong, medium, small). Once I have a better understanding of the content, credentials, and dependencies, we can all together establish a timeframe.\n Audit all sources of SimPy documentation (strong) Set an organization strategy for documentation (medium) In this stage, I will assess Diátaxis as a documentation system to organize SimPy content into four categories: How-To guides, Tutorials, References, and Explanations. Alternatively, I will evaluate a tagging system used in mkdocs that shows more scalability, particularly when searching documents that can fit into two or more categories. It is important that the proposed organization system is flexible to include new content such as the user guides. Classify documents based on the chosen organization strategy (strong) In this stage, I will organize content into categories complying with the organization strategy. Improve the SymPy site user experience (strong) In this stage, I will perform a survey introducing questions that can help us understand how end-users navigate, access, and consult the different types of documents of the SymPy site. Then, I will perform analytics to gather insights from the collected data and apply those recommendations to improve the SymPy site. To avoid content disruption of the original SymPy site, I recommend working on a development branch until deploying the last version. If time allows, I will explore if Sphinx can include Google Analytics to track the usability of the SimPy site. Test Sympy site (medium) In this stage, I will perform a final round of tests to assess the final version of the SymPy site. I recommend conducting another survey six months after the last release of the site to collect and compare valuable insights about its usability.  I am looking forward to contributing to SymPy. Please refer to my resume and portfolio for a more detailed description of my technical writing experience.\nSalomon Marquez\nPortfolio | LinkedIn | GitHub | Scholar\n 1. What ideas do you have for improving the SymPy documentation organization? and What sorts of challenges do you anticipate for this project and how will you work to overcome them? My proposal consists of five main stages. I provide my estimates in terms of effort (strong, medium, small). Once I have a better understanding of the content, credentials, and dependencies, we can all together establish a timeframe.\n  Set an organization strategy for documentation (medium effort)\n Ideas  Define the model to organize SymPy documentation. Create a documentation catalog.   Challenges  Discuss with mentors the best approach to organize information and define the guidelines so that the SymPy community can contribute to the documentation following this approach.   Proposed solutions  Analyze use cases of existing documentation following the Diátaxis organization system. For example, Numpy and Edo. Compare the Diátaxis system against other strategies that use a documentation catalog.      Audit all sources of SymPy documentation (strong effort)\n Ideas  Audit the four primary sources of information: SymPy Website, SymPy Documentation, SymPy Source Code, SymPy Wiki.   Challenges  Analyze SymPy documentation that includes over 243 .rst files and 576 Wiki pages.   Proposed solutions  Allocate more time to accomplish this stage of the project.      Classify documents based on the chosen organization strategy (medium effort)\n Ideas  Tag documents based on the organization model. Evaluate a tagging system used in mkdocs that shows more scalability, particularly when organizing documents that can fit into two or more categories.   Challenges  Test if Sphinx is compatible with a file tagging system.   Proposed solutions  Instead of tags, organize the information using folders in the SymPy doc directory. Work on a proof of concept in the Test environment of the SymPy site to implement the file tagging system.      Improve the SymPy site user experience (strong effort)\n Ideas  Perform a survey that can help us understand how end-users interact with the SymPy site. Gather valuable insights from collected data. Update the SymPy site to include the new documentation categories and apply the collected feedback to leverage the accessibility and usability of the site. Explore if Sphinx can include Google Analytics to track the usability of the SymPy site.   Challenges  Foster the participation of the SymPy community to answer the survey. Update the SymPy site integrating newly organized content and recommendations from the community.   Proposed solutions  Launch the survey using Google Forms because the community is already familiar with using Google groups. Set up the new Test Environment for the SymPy site at the early stages of the project.      Test SymPy site (medium effort)\n Ideas  Test the new SymPy site for readiness, accessibility, and usability. Launch to production the new site.   Challenges  No challenges identified       2. How would you interact with the SymPy community during Season of Docs? I have different points of contact I can use to interact with the SymPy community.\n I will become part of the community. As an end-user and contributor to SymPy, I will understand the mechanisms of interaction and logistics underneath making a PR, for example. My approach to a broader audience, such as the SymPy community, will be through the [SymPy Google group](SymPy Google group). Alternatively, I can use other communication channels such as Gitter. I will plan to have weekly sessions with mentors of the SymPy project to provide updates and check progress. I will suggest using an alternative communication channel such as Hangouts or Slack to keep constant communication with mentors. Communication should be straightforward because there is only one hour difference between our time zones.   3. Finally, we would like to ask if you would prefer to work 10 hours or 20 hours a week for this project. I can commit to an average of 10 to 15 hours per week to achieve the goals of this project.\n"});})();